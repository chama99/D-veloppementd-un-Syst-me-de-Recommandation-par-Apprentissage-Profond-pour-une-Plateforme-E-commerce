{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importations des bibliothèques"
      ],
      "metadata": {
        "id": "aXIBcKhuO29e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Reshape, Concatenate, Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "k6o8gUqzbUDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement des données"
      ],
      "metadata": {
        "id": "EXboMwnzO-YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données\n",
        "df_pro = pd.read_csv('Produits_Combines_2.csv')\n",
        "df_user = pd.read_csv('Utilisateurs_Tunisie_Updated.csv')\n",
        "df_rat = pd.read_csv('ratings.csv')"
      ],
      "metadata": {
        "id": "06niIKhSFYf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Affichage de la forme des DataFrames"
      ],
      "metadata": {
        "id": "rnfYmLNMPHh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_pro.shape)\n",
        "print(df_user.shape)\n",
        "print(df_rat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Z1we2tbrqn",
        "outputId": "79bce618-1ad0-4ba9-ec0f-b040d17d442d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4478, 13)\n",
            "(1000, 5)\n",
            "(5040, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rat.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6JJD9iFSAZGp",
        "outputId": "f55a7e50-086d-4337-a04f-f43d72c783f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  user_id  prod_id  rating   timestamp\n",
              "0  user_1     4451       5  2022-01-24\n",
              "1  user_1     2206       2  2022-06-01\n",
              "2  user_1     4125       5  2022-10-13\n",
              "3  user_1     3687       4  2022-03-22\n",
              "4  user_1     3877       5  2023-10-06"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4163452-d8ac-4aea-8828-5f33b4866011\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>prod_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_1</td>\n",
              "      <td>4451</td>\n",
              "      <td>5</td>\n",
              "      <td>2022-01-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_1</td>\n",
              "      <td>2206</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-06-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_1</td>\n",
              "      <td>4125</td>\n",
              "      <td>5</td>\n",
              "      <td>2022-10-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_1</td>\n",
              "      <td>3687</td>\n",
              "      <td>4</td>\n",
              "      <td>2022-03-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_1</td>\n",
              "      <td>3877</td>\n",
              "      <td>5</td>\n",
              "      <td>2023-10-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4163452-d8ac-4aea-8828-5f33b4866011')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4163452-d8ac-4aea-8828-5f33b4866011 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4163452-d8ac-4aea-8828-5f33b4866011');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b13e0817-c632-4663-9f49-4810673abe93\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b13e0817-c632-4663-9f49-4810673abe93')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b13e0817-c632-4663-9f49-4810673abe93 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_rat",
              "summary": "{\n  \"name\": \"df_rat\",\n  \"rows\": 5040,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"user_522\",\n          \"user_738\",\n          \"user_741\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prod_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1367,\n        \"min\": 1,\n        \"max\": 4457,\n        \"num_unique_values\": 2169,\n        \"samples\": [\n          1686,\n          1988,\n          2912\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1411,\n        \"samples\": [\n          \"2022-09-29\",\n          \"2020-09-04\",\n          \"2023-05-10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "r-AHpuLcBSTj",
        "outputId": "4367cfbf-a1a9-4584-f1d6-5065afc9bf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id_product                                              Title Marque  \\\n",
              "0           1                  SVR SEBIACLEAR GEL MOUSSANT 400ML    SVR   \n",
              "1           2  SVR SEBIACLEAR ACTIVE GEL CORRECTEUR INTENSIF ...    SVR   \n",
              "2           3                AVENE CLEANANCE GEL NETTOYANT 200ML  AVENE   \n",
              "3           4                 SVR SEBIACLEAR ACTIVE TEINTEE 40ML    SVR   \n",
              "4           5         SVR SEBIACLEAR GEL MOUSSANT RECHARGE 400ML    SVR   \n",
              "\n",
              "                                                 URL       Price  \\\n",
              "0  https://pharma-shop.tn/nettoyage/1536-svr-sebi...  41,098 TND   \n",
              "1  https://pharma-shop.tn/soins-specifiques/891-s...  38,229 TND   \n",
              "2  https://pharma-shop.tn/gel-creme-huile/1558-av...  36,346 TND   \n",
              "3  https://pharma-shop.tn/soins-teintes/5074-svr-...  37,130 TND   \n",
              "4  https://pharma-shop.tn/gel-creme-huile/11094-s...  38,758 TND   \n",
              "\n",
              "  Original Price Discount        Category  \\\n",
              "0     51,373 TND     -20%  pharma-shop.tn   \n",
              "1     47,786 TND     -20%  pharma-shop.tn   \n",
              "2     40,384 TND     -10%  pharma-shop.tn   \n",
              "3     46,413 TND     -20%  pharma-shop.tn   \n",
              "4     50,998 TND     -24%  pharma-shop.tn   \n",
              "\n",
              "                          Sub-Category  \\\n",
              "0  853-soins-peau-grasse-mixte-et-acne   \n",
              "1  853-soins-peau-grasse-mixte-et-acne   \n",
              "2  853-soins-peau-grasse-mixte-et-acne   \n",
              "3  853-soins-peau-grasse-mixte-et-acne   \n",
              "4  853-soins-peau-grasse-mixte-et-acne   \n",
              "\n",
              "                                          Image Path  \\\n",
              "0  C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...   \n",
              "1  C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...   \n",
              "2  C:\\Users\\broun\\downloaded_images\\avene-cleanan...   \n",
              "3  C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...   \n",
              "4  C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...   \n",
              "\n",
              "                                 Product Description  \\\n",
              "0  Description : Gel moussant Sébiaclear de SVR, ...   \n",
              "1  Pour les peaux sensibles à tendance acnéique, ...   \n",
              "2  Avène Cleanance Gel Nettoyant 200 ml est un ge...   \n",
              "3  Description : Pour les peaux sensibles à tenda...   \n",
              "4  Nettoyant sans savon qui purifie et désincrust...   \n",
              "\n",
              "                                  Product Main Image  \\\n",
              "0  https://pharma-shop.tn/15782-large_default/svr...   \n",
              "1  https://pharma-shop.tn/11978-large_default/svr...   \n",
              "2  https://pharma-shop.tn/19335-large_default/ave...   \n",
              "3  https://pharma-shop.tn/17535-large_default/svr...   \n",
              "4  https://pharma-shop.tn/17421-large_default/svr...   \n",
              "\n",
              "                     Manufacturer Logo  \n",
              "0  https://pharma-shop.tn/img/m/68.jpg  \n",
              "1  https://pharma-shop.tn/img/m/68.jpg  \n",
              "2  https://pharma-shop.tn/img/m/11.jpg  \n",
              "3  https://pharma-shop.tn/img/m/68.jpg  \n",
              "4  https://pharma-shop.tn/img/m/68.jpg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da9cc7c4-8194-43c1-b882-31ed563cde67\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_product</th>\n",
              "      <th>Title</th>\n",
              "      <th>Marque</th>\n",
              "      <th>URL</th>\n",
              "      <th>Price</th>\n",
              "      <th>Original Price</th>\n",
              "      <th>Discount</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Image Path</th>\n",
              "      <th>Product Description</th>\n",
              "      <th>Product Main Image</th>\n",
              "      <th>Manufacturer Logo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>SVR SEBIACLEAR GEL MOUSSANT 400ML</td>\n",
              "      <td>SVR</td>\n",
              "      <td>https://pharma-shop.tn/nettoyage/1536-svr-sebi...</td>\n",
              "      <td>41,098 TND</td>\n",
              "      <td>51,373 TND</td>\n",
              "      <td>-20%</td>\n",
              "      <td>pharma-shop.tn</td>\n",
              "      <td>853-soins-peau-grasse-mixte-et-acne</td>\n",
              "      <td>C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...</td>\n",
              "      <td>Description : Gel moussant Sébiaclear de SVR, ...</td>\n",
              "      <td>https://pharma-shop.tn/15782-large_default/svr...</td>\n",
              "      <td>https://pharma-shop.tn/img/m/68.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>SVR SEBIACLEAR ACTIVE GEL CORRECTEUR INTENSIF ...</td>\n",
              "      <td>SVR</td>\n",
              "      <td>https://pharma-shop.tn/soins-specifiques/891-s...</td>\n",
              "      <td>38,229 TND</td>\n",
              "      <td>47,786 TND</td>\n",
              "      <td>-20%</td>\n",
              "      <td>pharma-shop.tn</td>\n",
              "      <td>853-soins-peau-grasse-mixte-et-acne</td>\n",
              "      <td>C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...</td>\n",
              "      <td>Pour les peaux sensibles à tendance acnéique, ...</td>\n",
              "      <td>https://pharma-shop.tn/11978-large_default/svr...</td>\n",
              "      <td>https://pharma-shop.tn/img/m/68.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>AVENE CLEANANCE GEL NETTOYANT 200ML</td>\n",
              "      <td>AVENE</td>\n",
              "      <td>https://pharma-shop.tn/gel-creme-huile/1558-av...</td>\n",
              "      <td>36,346 TND</td>\n",
              "      <td>40,384 TND</td>\n",
              "      <td>-10%</td>\n",
              "      <td>pharma-shop.tn</td>\n",
              "      <td>853-soins-peau-grasse-mixte-et-acne</td>\n",
              "      <td>C:\\Users\\broun\\downloaded_images\\avene-cleanan...</td>\n",
              "      <td>Avène Cleanance Gel Nettoyant 200 ml est un ge...</td>\n",
              "      <td>https://pharma-shop.tn/19335-large_default/ave...</td>\n",
              "      <td>https://pharma-shop.tn/img/m/11.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>SVR SEBIACLEAR ACTIVE TEINTEE 40ML</td>\n",
              "      <td>SVR</td>\n",
              "      <td>https://pharma-shop.tn/soins-teintes/5074-svr-...</td>\n",
              "      <td>37,130 TND</td>\n",
              "      <td>46,413 TND</td>\n",
              "      <td>-20%</td>\n",
              "      <td>pharma-shop.tn</td>\n",
              "      <td>853-soins-peau-grasse-mixte-et-acne</td>\n",
              "      <td>C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...</td>\n",
              "      <td>Description : Pour les peaux sensibles à tenda...</td>\n",
              "      <td>https://pharma-shop.tn/17535-large_default/svr...</td>\n",
              "      <td>https://pharma-shop.tn/img/m/68.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>SVR SEBIACLEAR GEL MOUSSANT RECHARGE 400ML</td>\n",
              "      <td>SVR</td>\n",
              "      <td>https://pharma-shop.tn/gel-creme-huile/11094-s...</td>\n",
              "      <td>38,758 TND</td>\n",
              "      <td>50,998 TND</td>\n",
              "      <td>-24%</td>\n",
              "      <td>pharma-shop.tn</td>\n",
              "      <td>853-soins-peau-grasse-mixte-et-acne</td>\n",
              "      <td>C:\\Users\\broun\\downloaded_images\\svr-sebiaclea...</td>\n",
              "      <td>Nettoyant sans savon qui purifie et désincrust...</td>\n",
              "      <td>https://pharma-shop.tn/17421-large_default/svr...</td>\n",
              "      <td>https://pharma-shop.tn/img/m/68.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da9cc7c4-8194-43c1-b882-31ed563cde67')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da9cc7c4-8194-43c1-b882-31ed563cde67 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da9cc7c4-8194-43c1-b882-31ed563cde67');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e07bc349-4e1d-43aa-8a65-1ad30801d392\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e07bc349-4e1d-43aa-8a65-1ad30801d392')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e07bc349-4e1d-43aa-8a65-1ad30801d392 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_pro",
              "summary": "{\n  \"name\": \"df_pro\",\n  \"rows\": 4478,\n  \"fields\": [\n    {\n      \"column\": \"id_product\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1292,\n        \"min\": 1,\n        \"max\": 4478,\n        \"num_unique_values\": 4478,\n        \"samples\": [\n          3998,\n          3027,\n          1911\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2841,\n        \"samples\": [\n          \"SO'BIO PRECIEUX ARGAN PEAUX MATURES CREME ROSE...\",\n          \"LIRENE SAGE CREME MAINS 20% 75ML\",\n          \"SILVER CLEAR GEL NETTOYANT PURIFIANT 200ML\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Marque\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 207,\n        \"samples\": [\n          \"FARMAVANS\",\n          \"LA ROCHE POSAY\",\n          \"neutraderm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2883,\n        \"samples\": [\n          \"https://pharma-shop.tn/visage/10160-cerave-lait-hydratant-236-ml-3337875597395.html\",\n          \"https://pharma-shop.tn/masques-visage-et-gommage/9583-endocare-radiance-peel-mask-eclaircissant-56ml-8470001713506.html\",\n          \"https://pharma-shop.tn/memoire-et-concentration/1368-vital-phytothera-memoire-concentration-30-gelules-6192421100293.html\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2210,\n        \"samples\": [\n          \"22,071\\u00a0TND\",\n          \"20,304\\u00a0TND\",\n          \"14,399\\u00a0TND\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Price\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 857,\n        \"samples\": [\n          \"4,450\\u00a0TND\",\n          \"31,249\\u00a0TND\",\n          \"83,948\\u00a0TND\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Discount\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"-20%\",\n          \"-100%\",\n          \"-3%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"www.pharma-shop.tn\",\n          \"pharma-shop.tn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sub-Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"1083-soins-repigmentant\",\n          \"876-soins-des-cicatrices\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Image Path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2957,\n        \"samples\": [\n          \"C:\\\\Users\\\\broun\\\\downloaded_images\\\\eneomey-soft-cleanser.jpg\",\n          \"C:\\\\Users\\\\broun\\\\downloaded_images_complements_alimentaires\\\\vitonic-examens-45-gelules.webp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2645,\n        \"samples\": [\n          \"Description : CLARENIA\\u00a9 GEL NETTOYANT ECLAIRCISSANT DOUBLE ACTION \\u00e9limine les impuret\\u00e9s et l'exc\\u00e8s de s\\u00e9bum et corrige les troubles de pigmentation. Gr\\u00e2ce \\u00e0 ses actifs 100% naturels, la peau est parfaitement nettoy\\u00e9e, fra\\u00eeche et douce. Le teint parait plus clair et plus unifi\\u00e9. La peau retrouve une luminosit\\u00e9 parfaite. Avec sa haute teneur en agents hydratants ce nettoyant \\u00e0 base de savon ne fait pas s\\u00e9cher la peau. R\\u00e9sultat : au fil des applications la peau retrouve son \\u00e9clat. Conseils d\\u2019utilisation : A utiliser matin et soir\\u00a0: Mettez le gel dans les paumes de vos mains avec de l'eau. Il est pr\\u00e9f\\u00e9rable d\\u2019\\u00e9viter le contour des yeux. Rincer soigneusement. Composition : Eau purifi\\u00e9e, gel d\\u2019aloe vera, cocomidopropyl betaine, potassium cocoate, extrait de busserole (arbutine), glycerine v\\u00e9g\\u00e9tale, hydrolat de citron, coco glucoside, glyceryl oleate, huile d\\u2019amande douce, sel marin, allantoine, vitamine C, gomme xanthane, fragrance, alcool benzylique, acide dehydroacetique.\",\n          \"Mincivit Detox est un compl\\u00e9ment alimentaire \\u00e0 base des extraits secs de 7 plantes (Th\\u00e9 vert, Fenouil, Pissenlit, Piloselle, Desmodium, Artichaut et bouleau) reconnus par leur effet draineur et \\u00e9liminatoire des toxines qui 'attaquent aux probl\\u00e8mes de r\\u00e9tention d'au et purifient l'organisme pour affiner la silhouette. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Main Image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2883,\n        \"samples\": [\n          \"https://pharma-shop.tn/16006-large_default/cerave-lait-hydratant-236-ml.jpg\",\n          \"https://pharma-shop.tn/14999-large_default/endocare-radiance-peel-mask-eclaircissant-56ml.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Manufacturer Logo\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 201,\n        \"samples\": [\n          \"https://pharma-shop.tn/img/m/375.jpg\",\n          \"https://pharma-shop.tn/img/m/40.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nettoyage des Données\n"
      ],
      "metadata": {
        "id": "Rsomz9lfrhVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Suppression des Valeurs Manquantes"
      ],
      "metadata": {
        "id": "gRUW2n-X0z9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_pro.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bLPlx_hbymm",
        "outputId": "d234e114-ea1d-403e-8af7-277d5aa44090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_product                0\n",
            "Title                     0\n",
            "Marque                   27\n",
            "URL                       0\n",
            "Price                     0\n",
            "Original Price         2878\n",
            "Discount               2878\n",
            "Category                  0\n",
            "Sub-Category              0\n",
            "Image Path                0\n",
            "Product Description     213\n",
            "Product Main Image        0\n",
            "Manufacturer Logo        78\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_user.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nXVRGpRcnYb",
        "outputId": "6104652a-5247-42f8-b9d4-cb14394647a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                  0\n",
            "age                 0\n",
            "sexe                0\n",
            "ville               0\n",
            "historique_achat    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_rat.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pa0cbwWcp_R",
        "outputId": "0823935e-e1b9-499b-93b9-a420a78de31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_id      0\n",
            "prod_id      0\n",
            "rating       0\n",
            "timestamp    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacer les valeurs NaN dans la colonne 'Discount' par '0%'\n",
        "df_pro['Discount'] = df_pro['Discount'].fillna('0%')\n",
        "\n"
      ],
      "metadata": {
        "id": "IK_y_2h5r8ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacer les valeurs NaN dans la colonne 'Price_Original' par les valeurs de la colonne 'Price'\n",
        "df_pro['Original Price'] = df_pro['Original Price'].fillna(df_pro['Price'])"
      ],
      "metadata": {
        "id": "bRh_rl7osIua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "a7HLny1gsYbI",
        "outputId": "04f2a41e-4739-4dbc-e2b9-5abc7adede6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id_product               0\n",
              "Title                    0\n",
              "Marque                  27\n",
              "URL                      0\n",
              "Price                    0\n",
              "Original Price           0\n",
              "Discount                 0\n",
              "Category                 0\n",
              "Sub-Category             0\n",
              "Image Path               0\n",
              "Product Description    213\n",
              "Product Main Image       0\n",
              "Manufacturer Logo       78\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id_product</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Title</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Marque</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URL</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Original Price</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Discount</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sub-Category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image Path</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Product Description</th>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Product Main Image</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Manufacturer Logo</th>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour extraire le premier mot d'une chaîne\n",
        "def get_first_word(title):\n",
        "    return title.split()[0] if pd.notnull(title) else None\n",
        "\n",
        "# Remplacer les valeurs manquantes dans 'Marque' par le premier mot de 'Title'\n",
        "df_pro['Marque'] = df_pro['Marque'].fillna(df_pro['Title'].apply(get_first_word))\n",
        "\n",
        "# Vérifier les valeurs manquantes après le remplacement\n",
        "print(df_pro['Marque'].isnull().sum())\n",
        "\n",
        "# Afficher les premières lignes pour vérifier\n",
        "print(df_pro[['Title', 'Marque']].head(200))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1rpq-aaCQui",
        "outputId": "b2098656-bb12-4a7a-84ed-00760ca80587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                                 Title           Marque\n",
            "0                    SVR SEBIACLEAR GEL MOUSSANT 400ML              SVR\n",
            "1    SVR SEBIACLEAR ACTIVE GEL CORRECTEUR INTENSIF ...              SVR\n",
            "2                  AVENE CLEANANCE GEL NETTOYANT 200ML            AVENE\n",
            "3                   SVR SEBIACLEAR ACTIVE TEINTEE 40ML              SVR\n",
            "4           SVR SEBIACLEAR GEL MOUSSANT RECHARGE 400ML              SVR\n",
            "..                                                 ...              ...\n",
            "195    Debada Trousse soins visage peau mixte à grasse           Debada\n",
            "196  LIRENE ACID POWER REVITALIZING PERFECTING CREA...           Lirene\n",
            "197          NUXE REVE DE MIEL SURGRAS NETTOYANT 750ML             NUXE\n",
            "198            NATURA SIBERICA PEELING PURIFIANT ET...  NATURA SIBERICA\n",
            "199      GARNIER PURE ACTIVE NETTOYANT 3 EN 1 - 150 ML          GARNIER\n",
            "\n",
            "[200 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour extraire tous les mots sauf le premier d'une chaîne\n",
        "def get_rest_of_words(title):\n",
        "    if pd.notnull(title):\n",
        "        words = title.split()\n",
        "        return ' '.join(words[1:]) if len(words) > 1 else ''  # Si plus d'un mot, retourner tous sauf le premier\n",
        "    return None\n",
        "\n",
        "# Remplacer les valeurs manquantes dans 'Product Description' par le reste du titre sans le premier mot\n",
        "df_pro['Product Description'] = df_pro['Product Description'].fillna(df_pro['Title'].apply(get_rest_of_words))\n",
        "\n",
        "# Vérifier les valeurs manquantes après le remplacement\n",
        "print(df_pro['Product Description'].isnull().sum())\n",
        "\n",
        "# Afficher les premières lignes pour vérifier\n",
        "print(df_pro[['Title', 'Product Description']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU8mrd_-DdzQ",
        "outputId": "3c439d4d-b2a0-4a86-f12e-cb0256b4ecae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "                                               Title  \\\n",
            "0                  SVR SEBIACLEAR GEL MOUSSANT 400ML   \n",
            "1  SVR SEBIACLEAR ACTIVE GEL CORRECTEUR INTENSIF ...   \n",
            "2                AVENE CLEANANCE GEL NETTOYANT 200ML   \n",
            "3                 SVR SEBIACLEAR ACTIVE TEINTEE 40ML   \n",
            "4         SVR SEBIACLEAR GEL MOUSSANT RECHARGE 400ML   \n",
            "\n",
            "                                 Product Description  \n",
            "0  Description : Gel moussant Sébiaclear de SVR, ...  \n",
            "1  Pour les peaux sensibles à tendance acnéique, ...  \n",
            "2  Avène Cleanance Gel Nettoyant 200 ml est un ge...  \n",
            "3  Description : Pour les peaux sensibles à tenda...  \n",
            "4  Nettoyant sans savon qui purifie et désincrust...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_pro.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwJBhYOWDnOw",
        "outputId": "2767c5a3-cc9f-43cf-8be8-9594913f3230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_product              0\n",
            "Title                   0\n",
            "Marque                  0\n",
            "URL                     0\n",
            "Price                   0\n",
            "Original Price          0\n",
            "Discount                0\n",
            "Category                0\n",
            "Sub-Category            0\n",
            "Image Path              0\n",
            "Product Description     0\n",
            "Product Main Image      0\n",
            "Manufacturer Logo      78\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier s'il y a des valeurs NaN dans l'ensemble du DataFrame\n",
        "missing_values = df_pro.isnull().sum()\n",
        "\n",
        "# Afficher le nombre de valeurs NaN par colonne\n",
        "print(\"Nombre de valeurs NaN par colonne :\")\n",
        "print(missing_values)\n",
        "\n",
        "# Vérifier si le DataFrame contient au moins une valeur NaN\n",
        "if missing_values.any():\n",
        "    print(\"Le DataFrame contient des valeurs NaN.\")\n",
        "else:\n",
        "    print(\"Le DataFrame ne contient aucune valeur NaN.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewDtCAt6Fs2P",
        "outputId": "4322bae8-09a1-40d9-fb18-fd453612f12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de valeurs NaN par colonne :\n",
            "id_product              0\n",
            "Title                   0\n",
            "Marque                  0\n",
            "URL                     0\n",
            "Price                   0\n",
            "Original Price          0\n",
            "Discount                0\n",
            "Category                0\n",
            "Sub-Category            0\n",
            "Image Path              0\n",
            "Product Description     0\n",
            "Product Main Image      0\n",
            "Manufacturer Logo      78\n",
            "dtype: int64\n",
            "Le DataFrame contient des valeurs NaN.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Suppression des Doublons"
      ],
      "metadata": {
        "id": "G9t9P4Mz05zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier s'il y a des lignes dupliquées dans l'ensemble du DataFrame\n",
        "duplicates = df_pro.duplicated()\n",
        "\n",
        "# Afficher le nombre de doublons\n",
        "print(f\"Nombre de doublons : {duplicates.sum()}\")\n",
        "\n",
        "# Afficher les lignes dupliquées si elles existent\n",
        "if duplicates.any():\n",
        "    print(\"Lignes dupliquées :\")\n",
        "    print(df_pro[duplicates])\n",
        "else:\n",
        "    print(\"Aucune ligne dupliquée.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT-DlpgREGWk",
        "outputId": "15006987-6b70-4596-ce13-8abcd87614d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de doublons : 0\n",
            "Aucune ligne dupliquée.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier s'il y a des lignes dupliquées dans l'ensemble du DataFrame\n",
        "duplicates = df_rat.duplicated()\n",
        "\n",
        "# Afficher le nombre de doublons\n",
        "print(f\"Nombre de doublons : {duplicates.sum()}\")\n",
        "\n",
        "# Afficher les lignes dupliquées si elles existent\n",
        "if duplicates.any():\n",
        "    print(\"Lignes dupliquées :\")\n",
        "    print(df_rat[duplicates])\n",
        "else:\n",
        "    print(\"Aucune ligne dupliquée.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_oMRGJBEJe6",
        "outputId": "bb143b8a-c946-4fee-9e7f-1be70ed19af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de doublons : 0\n",
            "Aucune ligne dupliquée.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier s'il y a des lignes dupliquées dans l'ensemble du DataFrame\n",
        "duplicates = df_user.duplicated()\n",
        "\n",
        "# Afficher le nombre de doublons\n",
        "print(f\"Nombre de doublons : {duplicates.sum()}\")\n",
        "\n",
        "# Afficher les lignes dupliquées si elles existent\n",
        "if duplicates.any():\n",
        "    print(\"Lignes dupliquées :\")\n",
        "    print(df_user[duplicates])\n",
        "else:\n",
        "    print(\"Aucune ligne dupliquée.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2OoEel8EOzc",
        "outputId": "9bf6ff97-9a4d-4c89-ae9e-537dd0370f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de doublons : 0\n",
            "Aucune ligne dupliquée.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prétraitement des Données\n"
      ],
      "metadata": {
        "id": "SEpuI40V1neY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* cette section du code prépare les caractéristiques des produits en encodant les données catégorielles et textuelles en valeurs numériques, et en nettoyant et convertissant les prix ."
      ],
      "metadata": {
        "id": "fDLTWUb-X3gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_features = df_pro[['Price', 'Product Description','Marque']].copy()\n",
        "\n",
        "\n",
        "\n",
        "# Créer des instances de LabelEncoder\n",
        "marque_encoder = LabelEncoder()\n",
        "\n",
        "\n",
        "# Encoder la colonne 'Marque'\n",
        "product_features ['Marque'] = marque_encoder.fit_transform(df_pro['Marque'])\n",
        "\n",
        "product_features = pd.get_dummies(product_features, columns=['Product Description'])\n",
        "\n",
        "\n",
        "product_features['Price'] = product_features['Price'].str.replace('TND','').str.replace(',', '.').astype(float)\n"
      ],
      "metadata": {
        "id": "iPb5lD-4FZqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifier les noms des colonnes du DataFrame\n",
        "nouveaux_noms_colonnes = ['user_id', 'id_product', 'rating', 'timestamp']\n",
        "df_rat.columns = nouveaux_noms_colonnes\n",
        "\n",
        "prod_dataset = df_pro[['id_product', 'Title']]\n",
        "prod_dataset.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eK2-kgww_cFh",
        "outputId": "92ab839f-9b27-4cba-d4fd-fdcd7afb9f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id_product                                              Title\n",
              "0           1                  SVR SEBIACLEAR GEL MOUSSANT 400ML\n",
              "1           2  SVR SEBIACLEAR ACTIVE GEL CORRECTEUR INTENSIF ...\n",
              "2           3                AVENE CLEANANCE GEL NETTOYANT 200ML\n",
              "3           4                 SVR SEBIACLEAR ACTIVE TEINTEE 40ML\n",
              "4           5         SVR SEBIACLEAR GEL MOUSSANT RECHARGE 400ML"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccbd7f33-5b4e-446a-ac39-b16f69dc5b48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_product</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>SVR SEBIACLEAR GEL MOUSSANT 400ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>SVR SEBIACLEAR ACTIVE GEL CORRECTEUR INTENSIF ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>AVENE CLEANANCE GEL NETTOYANT 200ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>SVR SEBIACLEAR ACTIVE TEINTEE 40ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>SVR SEBIACLEAR GEL MOUSSANT RECHARGE 400ML</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccbd7f33-5b4e-446a-ac39-b16f69dc5b48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ccbd7f33-5b4e-446a-ac39-b16f69dc5b48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ccbd7f33-5b4e-446a-ac39-b16f69dc5b48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa956ede-c2b3-4f96-959a-26b62a610994\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa956ede-c2b3-4f96-959a-26b62a610994')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa956ede-c2b3-4f96-959a-26b62a610994 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "prod_dataset",
              "summary": "{\n  \"name\": \"prod_dataset\",\n  \"rows\": 4478,\n  \"fields\": [\n    {\n      \"column\": \"id_product\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1292,\n        \"min\": 1,\n        \"max\": 4478,\n        \"num_unique_values\": 4478,\n        \"samples\": [\n          3998,\n          3027,\n          1911\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2841,\n        \"samples\": [\n          \"SO'BIO PRECIEUX ARGAN PEAUX MATURES CREME ROSE...\",\n          \"LIRENE SAGE CREME MAINS 20% 75ML\",\n          \"SILVER CLEAR GEL NETTOYANT PURIFIANT 200ML\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Cette ligne fusionne ( **merge** ) deux DataFrames, df_rat (qui contient les évaluations des utilisateurs) et prod_dataset (qui contient les informations sur les produits), en un seul DataFrame appelé merged_dataset.\n",
        "* Le type de fusion utilisé est **inner** , ce qui signifie que seuls les enregistrements ayant des correspondances dans les deux DataFrames sont inclus dans le DataFrame final. Autrement dit, seuls les produits ayant des évaluations dans df_rat et des informations dans prod_dataset seront inclus.\n",
        "* La fusion est effectuée sur la colonne **id_product** , qui est présente dans les deux DataFrames. Cette colonne est utilisée comme clé pour associer les enregistrements des deux ensembles de données."
      ],
      "metadata": {
        "id": "m4Kgg4PQY77_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_dataset = pd.merge(df_rat, prod_dataset, how='inner', on='id_product')\n",
        "merged_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YkeuMDPqGOim",
        "outputId": "8c32ca3e-5b68-492f-da06-0a1812f7dc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    user_id  id_product  rating   timestamp  \\\n",
              "0    user_1        4451       5  2022-01-24   \n",
              "1    user_1        2206       2  2022-06-01   \n",
              "2  user_480        2206       4  2022-12-02   \n",
              "3  user_815        2206       1  2020-02-07   \n",
              "4    user_1        4125       5  2022-10-13   \n",
              "\n",
              "                                            Title  \n",
              "0  VITABIOTICS PERFECTIL PLATINIUM RADIANCE 30...  \n",
              "1         GAMARDE CREME DE DOUCHE APAISANTE 200ML  \n",
              "2         GAMARDE CREME DE DOUCHE APAISANTE 200ML  \n",
              "3         GAMARDE CREME DE DOUCHE APAISANTE 200ML  \n",
              "4              3 Chênes Huile Onagre 150 Capsules  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe6dea16-2b73-4a46-b306-cb8c199eb161\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>id_product</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_1</td>\n",
              "      <td>4451</td>\n",
              "      <td>5</td>\n",
              "      <td>2022-01-24</td>\n",
              "      <td>VITABIOTICS PERFECTIL PLATINIUM RADIANCE 30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_1</td>\n",
              "      <td>2206</td>\n",
              "      <td>2</td>\n",
              "      <td>2022-06-01</td>\n",
              "      <td>GAMARDE CREME DE DOUCHE APAISANTE 200ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_480</td>\n",
              "      <td>2206</td>\n",
              "      <td>4</td>\n",
              "      <td>2022-12-02</td>\n",
              "      <td>GAMARDE CREME DE DOUCHE APAISANTE 200ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_815</td>\n",
              "      <td>2206</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-02-07</td>\n",
              "      <td>GAMARDE CREME DE DOUCHE APAISANTE 200ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_1</td>\n",
              "      <td>4125</td>\n",
              "      <td>5</td>\n",
              "      <td>2022-10-13</td>\n",
              "      <td>3 Chênes Huile Onagre 150 Capsules</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe6dea16-2b73-4a46-b306-cb8c199eb161')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe6dea16-2b73-4a46-b306-cb8c199eb161 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe6dea16-2b73-4a46-b306-cb8c199eb161');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5c7c60aa-6eba-488e-8e4e-0de242ef1aaf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c7c60aa-6eba-488e-8e4e-0de242ef1aaf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5c7c60aa-6eba-488e-8e4e-0de242ef1aaf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "merged_dataset",
              "summary": "{\n  \"name\": \"merged_dataset\",\n  \"rows\": 5040,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"user_988\",\n          \"user_791\",\n          \"user_129\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_product\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1367,\n        \"min\": 1,\n        \"max\": 4457,\n        \"num_unique_values\": 2169,\n        \"samples\": [\n          1686,\n          1988,\n          2912\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1411,\n        \"samples\": [\n          \"2023-10-03\",\n          \"2022-08-15\",\n          \"2020-10-01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2169,\n        \"samples\": [\n          \"BIOMIMETIC SERUM PRE BASE TREATMENT GLOBAL...\",\n          \"SVR CLAIRIAL AMPOULE CONCENTRE ANTI TACHES 30ML\",\n          \"BYPHASSE LAIT CORPOREL NUTRITIF A L\\u2019HUILE...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un ensemble de données final raffiné avec un identifiant d'utilisateur unique, une combinaison de noms de films et leurs notes\n",
        "refined_dataset = merged_dataset.groupby(by=['user_id','id_product','Title'], as_index=False).agg({\"rating\": \"mean\"})\n",
        "refined_dataset.head(200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "qhFIC0YJGWce",
        "outputId": "f34601fe-b31b-4332-ed93-7abc61220e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      user_id  id_product                                              Title  \\\n",
              "0      user_1        2206            GAMARDE CREME DE DOUCHE APAISANTE 200ML   \n",
              "1      user_1        2628         TITANIA FOOT CARE BEURRE POUR PIEDS 250 ML   \n",
              "2      user_1        2671  K-REINE CRÈME DE MODELAGE MAINS Citron et Aloé...   \n",
              "3      user_1        3687                HELIOCARE ORAL CAPSULES 60 CAPSULES   \n",
              "4      user_1        3877                    NATURALIUM QUICK DRAINEUR 300ML   \n",
              "..        ...         ...                                                ...   \n",
              "195  user_132        4147                         FERTILIS FEMME 120 GÉLULES   \n",
              "196  user_132        4331                       FORTIMEL EXTRA VANILLE 200ML   \n",
              "197  user_133          90    FILORGA AGE-PURIFY FLUID DOUBLE CORRECTION 50ML   \n",
              "198  user_133        1913                DUCRAY KELUAL DS CREME APAISANTE...   \n",
              "199  user_133        1922    URIAGE ROSELIANE CREME RICHE ANTI ROUGEURS,50ML   \n",
              "\n",
              "     rating  \n",
              "0       2.0  \n",
              "1       3.0  \n",
              "2       2.0  \n",
              "3       4.0  \n",
              "4       5.0  \n",
              "..      ...  \n",
              "195     5.0  \n",
              "196     4.0  \n",
              "197     3.0  \n",
              "198     5.0  \n",
              "199     1.0  \n",
              "\n",
              "[200 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ab34ab3-44f6-49c2-8416-fd3d7e321720\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>id_product</th>\n",
              "      <th>Title</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user_1</td>\n",
              "      <td>2206</td>\n",
              "      <td>GAMARDE CREME DE DOUCHE APAISANTE 200ML</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user_1</td>\n",
              "      <td>2628</td>\n",
              "      <td>TITANIA FOOT CARE BEURRE POUR PIEDS 250 ML</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user_1</td>\n",
              "      <td>2671</td>\n",
              "      <td>K-REINE CRÈME DE MODELAGE MAINS Citron et Aloé...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user_1</td>\n",
              "      <td>3687</td>\n",
              "      <td>HELIOCARE ORAL CAPSULES 60 CAPSULES</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>user_1</td>\n",
              "      <td>3877</td>\n",
              "      <td>NATURALIUM QUICK DRAINEUR 300ML</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>user_132</td>\n",
              "      <td>4147</td>\n",
              "      <td>FERTILIS FEMME 120 GÉLULES</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>user_132</td>\n",
              "      <td>4331</td>\n",
              "      <td>FORTIMEL EXTRA VANILLE 200ML</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>user_133</td>\n",
              "      <td>90</td>\n",
              "      <td>FILORGA AGE-PURIFY FLUID DOUBLE CORRECTION 50ML</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>user_133</td>\n",
              "      <td>1913</td>\n",
              "      <td>DUCRAY KELUAL DS CREME APAISANTE...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>user_133</td>\n",
              "      <td>1922</td>\n",
              "      <td>URIAGE ROSELIANE CREME RICHE ANTI ROUGEURS,50ML</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ab34ab3-44f6-49c2-8416-fd3d7e321720')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ab34ab3-44f6-49c2-8416-fd3d7e321720 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ab34ab3-44f6-49c2-8416-fd3d7e321720');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b95dbba6-dbb3-4f80-9776-3660fe183684\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b95dbba6-dbb3-4f80-9776-3660fe183684')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b95dbba6-dbb3-4f80-9776-3660fe183684 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "refined_dataset",
              "summary": "{\n  \"name\": \"refined_dataset\",\n  \"rows\": 5034,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"user_568\",\n          \"user_762\",\n          \"user_765\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_product\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1366,\n        \"min\": 1,\n        \"max\": 4457,\n        \"num_unique_values\": 2169,\n        \"samples\": [\n          1688,\n          3458,\n          1416\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2169,\n        \"samples\": [\n          \"PETITE MAISON DEMAQUILLANT YEUX BIPHASES\",\n          \"ECRINAL SOIN CROISSANCE & RESISTANCE 10 ML\",\n          \"SENSILIS EAU MICELLAIRE [AR] PEAU SENSIBLES 400ML\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4252274016102706,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.0,\n          3.5,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "refined_dataset['user_id'] = refined_dataset['user_id'].astype('category').cat.codes\n",
        "\n",
        "\n",
        "n_users =refined_dataset['user_id'].nunique()\n",
        "n_produits = refined_dataset['id_product'].nunique()\n",
        "min_rating = min(refined_dataset['rating'])\n",
        "max_rating = max(refined_dataset['rating'])"
      ],
      "metadata": {
        "id": "1IFyo9ZsRW2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refined_dataset.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtlR3AZnSd1E",
        "outputId": "c093d333-2fe5-4401-efc9-50a0155df0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['user_id', 'id_product', 'Title', 'rating'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge rating data with product features\n",
        "merged_data = pd.merge(refined_dataset,product_features,left_on='id_product', right_index=True)\n",
        "\n",
        "merged_data.columns"
      ],
      "metadata": {
        "id": "xKmJOUNZI98M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9486fbfa-3eb8-40dc-96ae-d3647fe191b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['user_id', 'id_product', 'Title', 'rating', 'Price', 'Marque',\n",
              "       'Product Description_ ', 'Product Description_    ',\n",
              "       'Product Description_   Composition:AQUA (WATER), CITRIC ACID, HYDROXYPROPYL STARCH PHOSPHATE, ALPHA-ARBUTIN, ASCORBYLGLUCOSIDE, AMINOMETHYL PROPANOL, ALCOHOL DENAT., SODIUM HYDROXIDE, DICAPRYLYL CARBONATE, PHENOXYETHANOL, METHYLPARABEN, PROPYLPARABEN, GLYCYRRHIZA GLABRA (LICORICE) ROOT EXTRACT, SODIUM METABISULFITE, ETHYLPARABEN, TOCOPHEROL.  ConseIl d'Utilisation:  Il est recommandé d'Appliquer UNITONE 4 reveal serum 1 à 2 fois par jour sur les zones à traiter. Peut être utilisé le jour.',\n",
              "       'Product Description_  DESCRIPTION :  Adoptez une « beauty routine » grâce à notre nouvelle gamme de crèmes pour le visage : 24H et Q10. La crème de jour BYPHASSE lift instant associe la coenzyme Q10, un actif raffermissant efficace avec l’acide hyaluronique qui améliore l’élasticité de la peau.  Résultat beauté : Votre peau est douce et lisse, elle retrouve une nouvelle jeunesse.  Les plus du produit :  • acide hyaluronique  • SPF10  • texture légère  CONSEILS D’UTILISATION :  Appliquer le matin sur le visage et le cou sur une peau parfaitement propre.  COMPOSITION :  Aqua, Isohexadecane, Caprylic/Capric Triglyceride, Ethylhexyl Methoxycinnamate, Cetearyl Alcohol, Cetyl Alcohol, Glyceryl Stearate, Dimethicone, Phenoxyethanol, PEG-75 Stearate, Parfum, Xanthan Gum, Propylene Glycol, Ceteth-20, Steareth-20, Imidazolidinyl Urea, Ethylhexylglycerin, Butyrospermum Parkii Butter, BHT, Tetrasodium Edta, Ubiquinone, Citric Acid, Sodium Hyaluronate, Hydrolyzed Millet, Diazolidinyl Urea,Linalool, Citronellol, Geraniol, Butylphenyl Methylpropional, Benzyl Salicylate, Hydroxycitronellal, Alpha-Isomethyl Ionone, Limonene, Benzyl Alcohol, Cinnamyl Alcohol, Benzyl Benzoate, Amyl Cinnamal. ',\n",
              "       ...\n",
              "       'Product Description_Énergie.Acerola.Vitamine C.100% d'origine naturelle.Excellent goût fruits rouges.Energie Acérola contient exclusivement de la vitamine C 100% d'origine naturelle qui est utilisée pour ses propriétés tonifiantes et fortifiantes.Energie Acérola est recommandé aux femmes, aux hommes et aux enfants dès l'âge de 8 ans souhaitant une réponse immédiate à leur fatigue passagère.L'acérola est une source naturelle remarquable de vitamine C qui vous en apportera 20 à 30 fois plus que l'orange.Dès que la sensation de fatigue se fait sentir, Énergie Acérola vous procure Tonus et Vitalité tout au long de la journée avec seulement 1 comprimé à croquer par jour au très bon goût Fruits rouges.De part sa richesse en vitamine C 100% d'origine naturelle, Énergie Acérola est également une véritable source d'anti-oxydants.Fabriqué en U.E. Croquer un comprimé par jour, le matin. Cure de 60 jours. Se conformer aux conseils d’utilisation. Ne pas dépasser la dose journalière conseillée. Ce produit est déconseillé aux femmes enceintes ou qui allaitent et aux enfants de moins de 8 ans sans avis médical. Tenir hors de portée des jeunes enfants. Un complément alimentaire ne doit pas se substituer à une alimentation variée et équilibrée et à un mode de vie sain. N'hésitez pas à demander conseil à votre pharmacien. Edulcorants (sorbitol, aspartame), Extrait sec d'acérola titré à 25% en vitamine C naturelle, Colorant alimentaire (extrait de jus de betterave, maltodextrine). Anti-agglomérants (stearate de magnesium, silice colloïdale), Arôme naturel fruits rouges.',\n",
              "       'Product Description_Étant donné que les activités cellulaires dépendent de l'énergie, la coenzyme Q10 est essentielle au fonctionnement de presque toutes les cellules.La coenzyme Q10 est une enzyme qui se produit naturellement dans le corps mais à mesure que nous vieillissons et que nos carences nutritionnelles augmentent, elle est réduite dans notre corps, elle est donc devenue un complément nutritionnel important.Les raisons sont multiples mais les principales sont qu'elle augmente la vitalité, l'énergie et favorise le fonctionnement des muscles et du cœur.L'oxydation se produit continuellement dans le corps, créant les conditions pour que nous ayons des maladies.Cela peut provenir à la fois de la digestion des aliments que nous mangeons, mais aussi de l'exercice, des radiations, de la pollution, de l'empoisonnement à l'alcool ou aux métaux lourds, des infections, etc.Natures Aid Coenzyme Q10, agit comme un puissant antioxydant contre les radicaux libres et nous protège contre l'oxydation.',\n",
              "       'Product Description_émulsion cheratoregolatrice  particulièrement adaptée au traitement intensif des calos (mains  pieds  coudes  genoux) et épaississement de l'ongle.élimine la peau écailleuse en améliorant l'élasticité et la souplesse de la peau.réduire la cohésion cellulaire entre les cornéocytes des couches plus profondes du stratum corneum  favorisant la desquamation.hydrate efficacement la peau et favorise le renouvellement des cellules  réduit la peau de l'épaisseur. Appliquer la crème deux fois par jour sur la zone affectée  puis masser vigoureusement quelques minutes pour éliminer la kératine.',\n",
              "       'Product Description_– Brûlures d’estomac– Remontées acides– Reflux Gastro Œsophagien Contre indiqué chez l'enfant 2 gélules par jour Bicarbonate de sodium, Extrait sec de bardane, Extrait sec de réglisse, Extrait sec de curcuma',\n",
              "       'Product Description_– Hydratation et nutrition de la peau et des phanères– Rajeunissement de la peau– Effet anti-âge , antioxydant 2 capsules par jour ',\n",
              "       'Product Description_– SOSKIN D-WHITE, concentré en hexylresorcinol, cible l’origine des phénomènes d’hyperpigmentation. Aide à réduire l’apparence des taches visibles en surface et prévient l’apparition des taches naissantes.– Huile de jojoba. La peau est intensément nourrie pour retrouver souplesse et douceur. Procure une hydratation optimale et convient aux zones sensibles. La peau retrouve éclat, uniformité et souplesse. Texture onctueuse et généreuse, pénètre rapidement.Contenance : 250mlFabriqué en France. Testé dermatologiquement. Appliquer matin et/ou soir quotidiennement sur les zones concernées. Usage externe, ne pas utiliser chez la femme enceinte ou allaitante. Il est recommandé d’appliquer une protection solaire durant la journée (FPS30 minimum). Aqua/Water/Eau, Caprylic/Capric Triglyceride, Octyldodecanol, Niacinamide, Propylene Glycol, Glycerin, Sodium Acrylate/Sodium Acryloyldimethyl Taurate Copolymer, Hexylresorcinol, Stearyl Alcohol, Glyceryl Stearate, Phenoxyethanol, Macadamia Integrifolia Seed Oil, Simmondsia Chinensis (Jojoba) Seed Oil, Prunus Amygdalus Dulcis (Sweet Almond) Oil, Persea Gratissima (Avocado) Oil, Polyacrylate-13, Polyisobutene, Ethylhexylglycerin, Disodium EDTA, Parfum/Fragrance, Alpha-Arbutin, Polysorbate 20, Citronellol, Limonene, Linalool.',\n",
              "       'Product Description_– Une gélule le matin et une gélule le soir. – Déconseillé aux femmes enceintes et allaitantes.– Pas d’effets secondaires.– Risque d’allergie à cette plante.– La prise d’artichaut est contre indiquée en cas d’obstruction des voies biliaires, calculs biliaires.– A forte dose : chlostasique, hyperoestrogémie.– Ingestion de toute la boite : faire un lavement gastrique.',\n",
              "       'Product Description_• Capte les sucres et les graisses des aliments.• Augmente la combustion des lipides et la thermogenèse.• Eliminer l’excès d’eau dans l’organisme.  Thé vert, nopal, guarana, queue de cerise',\n",
              "       'Product Description_• Démaquille tout en douceur visage, yeux et lèvres en un seul geste. • Nettoie délicatement les Impuretés et toutes traces de maquillage waterproof même les plus soutenus. • Sa formule enrichie en collagène comble les rides et ridules et redonne un aspect rebondi • Hydrate en douceur et prévient le vieillissement cutané *Parfaitement démaquillé votre peau est aussi propre, hydraté, apaisé et incroyablement douce.',\n",
              "       'Product Description_⦁ CREME RICHE HYDRATANTE ⦁ Active l’hydratation naturelle ⦁ Renforce la barrière cutanée ⦁ Texture onctueuse ⦁ Confort immédiat et durable ⦁ Peaux Sèches à très sèches  Composition : Glycerine, Aquaxyl, Pentavitin, Niacinamide, Acide hyaluronique, Beurre de karité, Huile d’amande douce .'],\n",
              "      dtype='object', length=2797)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Division de données\n"
      ],
      "metadata": {
        "id": "oHAYAh1DGlBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize labels (y)\n",
        "scaler = MinMaxScaler()\n",
        "merged_data['rating'] = scaler.fit_transform(merged_data[['rating']])\n",
        "\n",
        "# Prepare features and labels\n",
        "X = merged_data[['user_id','id_product']].values\n",
        "y = merged_data['rating'].values\n",
        "content_features = merged_data.drop(columns=['user_id', 'id_product', 'Title', 'rating']).values\n",
        "\n",
        "# Convert data to float32\n",
        "X = X.astype('float32')\n",
        "y = y.astype('float32')\n",
        "content_features = content_features.astype('float32')\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test, content_train, content_test = train_test_split(X, y, content_features, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encoders for user and product IDs\n",
        "user_enc = LabelEncoder()\n",
        "item_enc = LabelEncoder()\n",
        "\n",
        "# Fit the encoders on the user and product IDs\n",
        "user_enc.fit(refined_dataset['user_id'])\n",
        "item_enc.fit(refined_dataset['id_product'])\n",
        "\n",
        "\n",
        "# Transform the user and product IDs in the training set\n",
        "X_train[:, 0] = user_enc.transform(X_train[:, 0])\n",
        "X_train[:, 1] = item_enc.transform(X_train[:, 1])\n",
        "X_test[:, 0] = user_enc.transform(X_test[:, 0])\n",
        "X_test[:, 1] = item_enc.transform(X_test[:, 1])\n",
        "\n",
        "# Normalisation des labels\n",
        "y_train = (y_train - min_rating) / (max_rating - min_rating)\n",
        "y_test = (y_test - min_rating) / (max_rating - min_rating)\n"
      ],
      "metadata": {
        "id": "OO8GUy2xQKAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(n_users, n_produits, n_factors, content_feature_size):\n",
        "    user_input = Input(shape=(1,), name='user_input')\n",
        "    produit_input = Input(shape=(1,), name='produit_input')\n",
        "    content_input = Input(shape=(content_feature_size,), name='content_input')\n",
        "\n",
        "    user_embedding = Embedding(n_users, n_factors, input_length=1)(user_input)\n",
        "    produit_embedding = Embedding(n_produits, n_factors, input_length=1)(produit_input)\n",
        "\n",
        "    user_vecs = Flatten()(user_embedding)\n",
        "    produit_vecs = Flatten()(produit_embedding)\n",
        "\n",
        "    input_vecs = Concatenate()([user_vecs, produit_vecs, content_input])\n",
        "\n",
        "    x = Dense(256, kernel_initializer='he_normal')(input_vecs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Dense(128, kernel_initializer='he_normal')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Dense(64, kernel_initializer='he_normal')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Dense(32, kernel_initializer='he_normal')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Dense(16, kernel_initializer='he_normal')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    # Couche de sortie pour la régression\n",
        "    x = Dense(1)(x)\n",
        "    x= Activation('sigmoid')(x)\n",
        "\n",
        "\n",
        "\n",
        "    model = Model(inputs=[user_input, produit_input, content_input], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "IGBNaWeRTE9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KFold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "k_fold_histories = []\n",
        "mse_scores = []\n",
        "mae_scores = []\n",
        "\n",
        "content_feature_size = content_features.shape[1]"
      ],
      "metadata": {
        "id": "sAICgzPpTQBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "# Initialisation\n",
        "best_model_history = None\n",
        "best_val_mse = float('inf')\n",
        "best_model_index = -1\n",
        "mse_scores = []\n",
        "mae_scores = []\n",
        "k_fold_histories = []\n",
        "train_mse_scores = []\n",
        "\n",
        "\n",
        "# Boucle sur les plis\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    # Préparation des données avec les caractéristiques supplémentaires\n",
        "    X_train_array_fold = [X_train_fold[:, 0], X_train_fold[:, 1], content_train[train_index]]\n",
        "    X_val_array_fold = [X_val_fold[:, 0], X_val_fold[:, 1], content_train[val_index]]\n",
        "\n",
        "    # Création du modèle\n",
        "    model = create_model(n_users, n_produits, n_factors=150, content_feature_size=content_feature_size)\n",
        "\n",
        "    # Early Stopping, ModelCheckpoint et ReduceLROn\n",
        "    Plateau\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint(f'best_model_fold_{i+1}.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "    # Ajustement du taux d'apprentissage\n",
        "    optimizer = Adam(learning_rate=0.0001)  # Réajuster le taux d'apprentissage\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "    # Entraînement\n",
        "    history = model.fit(\n",
        "        x=X_train_array_fold,\n",
        "        y=y_train_fold,\n",
        "        batch_size=64,  # Augmenter la taille du batch\n",
        "        epochs=200,\n",
        "        verbose=1,\n",
        "        validation_data=(X_val_array_fold, y_val_fold),\n",
        "        callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "    )\n",
        "\n",
        "    k_fold_histories.append(history)\n",
        "\n",
        "    # Prédictions pour le fold actuel\n",
        "\n",
        "    train_predictions = model.predict(X_train_array_fold)\n",
        "\n",
        "    val_predictions = model.predict(X_val_array_fold)\n",
        "    val_mse = mean_squared_error(y_val_fold, val_predictions)\n",
        "    val_mae = mean_absolute_error(y_val_fold, val_predictions)\n",
        "    train_mse = mean_squared_error(y_train_fold, train_predictions)\n",
        "    train_mse_scores.append(train_mse)\n",
        "\n",
        "    mse_scores.append(val_mse)\n",
        "    mae_scores.append(val_mae)\n",
        "\n",
        "    # Affichage des pertes pour ce fold\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['mse'], label='Train MSE')\n",
        "    plt.plot(history.history['val_mse'], label='Val MSE')\n",
        "    plt.title(f'Model MSE for Fold {i+1}')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Correcting the file extension for loading the best model\n",
        "best_model_index = np.argmin([min(history.history['val_loss']) for history in k_fold_histories])\n",
        "best_model_path = f'best_model_fold_{best_model_index+1}.keras'  # Corrected extension\n",
        "best_model = tf.keras.models.load_model(best_model_path)\n",
        "\n",
        "# Affichage des pertes pour le meilleur modèle\n",
        "best_history = k_fold_histories[best_model_index]\n",
        "plt.figure()\n",
        "plt.plot(best_history.history['loss'], label='Train Loss')\n",
        "plt.plot(best_history.history['val_loss'], label='Val Loss')\n",
        "plt.title(f'Best Model Loss for Fold {best_model_index+1}')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Train MSE scores: {train_mse_scores}\")\n",
        "print(f\"Validation MSE scores: {mse_scores}\")\n",
        "print(f\"Validation MAE scores: {mae_scores}\")\n",
        "print(f\"Mean Train MSE: {np.mean(train_mse_scores)}\")\n",
        "print(f\"Mean Validation MSE: {np.mean(mse_scores)}\")\n",
        "print(f\"Mean Validation MAE: {np.mean(mae_scores)}\")\n",
        "print(f\"Best model path: {best_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gRElMnZ5TGcq",
        "outputId": "8d53552b-ec93-497e-ef4d-75b32143df12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3579 - mse: 0.3579\n",
            "Epoch 1: val_loss improved from inf to 0.06796, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.3551 - mse: 0.3551 - val_loss: 0.0680 - val_mse: 0.0680 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2371 - mse: 0.2371\n",
            "Epoch 2: val_loss improved from 0.06796 to 0.03345, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.2352 - mse: 0.2352 - val_loss: 0.0335 - val_mse: 0.0335 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1571 - mse: 0.1571\n",
            "Epoch 3: val_loss improved from 0.03345 to 0.02867, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1563 - mse: 0.1563 - val_loss: 0.0287 - val_mse: 0.0287 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1127 - mse: 0.1127\n",
            "Epoch 4: val_loss improved from 0.02867 to 0.02704, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1127 - mse: 0.1127 - val_loss: 0.0270 - val_mse: 0.0270 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1064 - mse: 0.1064\n",
            "Epoch 5: val_loss improved from 0.02704 to 0.02626, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.1059 - mse: 0.1059 - val_loss: 0.0263 - val_mse: 0.0263 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0739 - mse: 0.0739\n",
            "Epoch 6: val_loss improved from 0.02626 to 0.02575, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0746 - mse: 0.0746 - val_loss: 0.0257 - val_mse: 0.0257 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0722 - mse: 0.0722\n",
            "Epoch 7: val_loss improved from 0.02575 to 0.02546, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0722 - mse: 0.0722 - val_loss: 0.0255 - val_mse: 0.0255 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0588 - mse: 0.0588\n",
            "Epoch 8: val_loss improved from 0.02546 to 0.02529, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0588 - mse: 0.0588 - val_loss: 0.0253 - val_mse: 0.0253 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0552 - mse: 0.0552\n",
            "Epoch 9: val_loss improved from 0.02529 to 0.02517, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0553 - mse: 0.0553 - val_loss: 0.0252 - val_mse: 0.0252 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0521 - mse: 0.0521\n",
            "Epoch 10: val_loss improved from 0.02517 to 0.02508, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0522 - mse: 0.0522 - val_loss: 0.0251 - val_mse: 0.0251 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0460 - mse: 0.0460\n",
            "Epoch 11: val_loss improved from 0.02508 to 0.02502, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0460 - mse: 0.0460 - val_loss: 0.0250 - val_mse: 0.0250 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 12: val_loss improved from 0.02502 to 0.02497, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0250 - val_mse: 0.0250 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0463 - mse: 0.0463\n",
            "Epoch 13: val_loss improved from 0.02497 to 0.02494, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 14: val_loss improved from 0.02494 to 0.02491, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 15: val_loss improved from 0.02491 to 0.02488, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 16: val_loss improved from 0.02488 to 0.02485, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 17: val_loss improved from 0.02485 to 0.02484, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 18: val_loss improved from 0.02484 to 0.02482, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0339 - mse: 0.0339\n",
            "Epoch 19: val_loss improved from 0.02482 to 0.02481, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 20: val_loss improved from 0.02481 to 0.02479, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0345 - mse: 0.0345\n",
            "Epoch 21: val_loss improved from 0.02479 to 0.02478, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 22: val_loss improved from 0.02478 to 0.02477, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 23: val_loss improved from 0.02477 to 0.02476, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 24: val_loss improved from 0.02476 to 0.02476, saving model to best_model_fold_1.keras\n",
            "\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 25: val_loss improved from 0.02476 to 0.02476, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 2.0000e-05\n",
            "Epoch 26/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 26: val_loss improved from 0.02476 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 2.0000e-05\n",
            "Epoch 27/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 27: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 2.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0346 - mse: 0.0346\n",
            "Epoch 28: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 2.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 29: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 2.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 30: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 31: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 32: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 33: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 34: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 35: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 36: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 37/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 37: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 38/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 38: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 39/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 39: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 40/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 40: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 41/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 41: val_loss improved from 0.02475 to 0.02475, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 42/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 42: val_loss improved from 0.02475 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 43/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 43: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 44/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 44: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 45/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0345 - mse: 0.0345\n",
            "Epoch 45: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 46/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 46: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 47/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 47: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 48/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 48: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 49/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 49: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 50/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 50: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 51/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 51: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 52/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 52: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 53/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 53: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 54/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 54: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 55/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 55: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 56/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 56: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 57/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 57: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 58/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 58: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 59/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 59: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 60/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 60: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 61/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 61: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 62/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 62: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 63/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 63: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 64/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0308 - mse: 0.0308\n",
            "Epoch 64: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 65/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 65: val_loss improved from 0.02474 to 0.02474, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 66/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 66: val_loss improved from 0.02474 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 67/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 67: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 68/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0279 - mse: 0.0279\n",
            "Epoch 68: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 69/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 69: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 70/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0279 - mse: 0.0279\n",
            "Epoch 70: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 71/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 71: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 72/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0322 - mse: 0.0322\n",
            "Epoch 72: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 73/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0267 - mse: 0.0267\n",
            "Epoch 73: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 74/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 74: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 75/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 75: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 76/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 76: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 77/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 77: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 78/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 78: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 79/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 79: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 80/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 80: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 81/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 81: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 82/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 82: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 83/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 83: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 84/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 84: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 85/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0295 - mse: 0.0295\n",
            "Epoch 85: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 86/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 86: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 87/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 87: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 88/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 88: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 89/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 89: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 90/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 90: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 91/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 91: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 92/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 92: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 93/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 93: val_loss improved from 0.02473 to 0.02473, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 94/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 94: val_loss improved from 0.02473 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 95/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 95: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 96/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 96: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 97/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 97: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 98/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 98: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 99/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 99: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 100/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 100: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 101/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 101: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 102/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 102: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 103/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 103: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 104/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 104: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 105/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 105: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 106: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 107: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 108: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 109: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 110: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 111: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 112: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 113: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 114: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 115: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 116: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 117: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 118: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 119: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 120: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 121: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 122: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 123: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 124: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0295 - mse: 0.0295\n",
            "Epoch 125: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 126: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0321 - mse: 0.0321\n",
            "Epoch 127: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 128: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0261 - mse: 0.0261\n",
            "Epoch 129: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 130: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 131: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 132: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 133: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 134: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 135: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 136: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 137: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 138: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 139: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 140: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 141: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 142: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 143: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 144: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 145: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 146/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0271 - mse: 0.0271\n",
            "Epoch 146: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 147/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 147: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 148/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 148: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 149/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 149: val_loss improved from 0.02472 to 0.02472, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 150/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0261 - mse: 0.0261\n",
            "Epoch 150: val_loss improved from 0.02472 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 151/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 151: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 152/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 152: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 153/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 153: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 154/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 154: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 155/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 155: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 156/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 156: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 157/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 157: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 158/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 158: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 159/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 159: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 160/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 160: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 161/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 161: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 162/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 162: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 163/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 163: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 164/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 164: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 165/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 165: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 166/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 166: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 167/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 167: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 168/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 168: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 169/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 169: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 170/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 170: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 171/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 171: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 172/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 172: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 173/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 173: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 174/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 174: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 175/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 175: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 176/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 176: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 177/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 177: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 178/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 178: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 179/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 179: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 180/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 180: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 181/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 181: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 182/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 182: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 183/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 183: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 184/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 184: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 185/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 185: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 186/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0261 - mse: 0.0261\n",
            "Epoch 186: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 187/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 187: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 188/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 188: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 189/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 189: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 190/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 190: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 191/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 191: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 192/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 192: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 193/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 193: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 194/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 194: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 195/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 195: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 196/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 196: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 197/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 197: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 198/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 198: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 199/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 199: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 200/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 200: val_loss improved from 0.02471 to 0.02471, saving model to best_model_fold_1.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 200.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5411 - mse: 0.5411\n",
            "Epoch 1: val_loss improved from inf to 0.06183, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.5370 - mse: 0.5370 - val_loss: 0.0618 - val_mse: 0.0618 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3105 - mse: 0.3105\n",
            "Epoch 2: val_loss improved from 0.06183 to 0.03624, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.3097 - mse: 0.3097 - val_loss: 0.0362 - val_mse: 0.0362 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2106 - mse: 0.2106\n",
            "Epoch 3: val_loss improved from 0.03624 to 0.02950, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.2099 - mse: 0.2099 - val_loss: 0.0295 - val_mse: 0.0295 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1451 - mse: 0.1451\n",
            "Epoch 4: val_loss improved from 0.02950 to 0.02678, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.1442 - mse: 0.1442 - val_loss: 0.0268 - val_mse: 0.0268 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1140 - mse: 0.1140\n",
            "Epoch 5: val_loss improved from 0.02678 to 0.02492, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1135 - mse: 0.1135 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0989 - mse: 0.0989\n",
            "Epoch 6: val_loss improved from 0.02492 to 0.02370, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0237 - val_mse: 0.0237 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0896 - mse: 0.0896\n",
            "Epoch 7: val_loss improved from 0.02370 to 0.02318, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.0232 - val_mse: 0.0232 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m48/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0755 - mse: 0.0755\n",
            "Epoch 8: val_loss improved from 0.02318 to 0.02276, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0750 - mse: 0.0750 - val_loss: 0.0228 - val_mse: 0.0228 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0628 - mse: 0.0628\n",
            "Epoch 9: val_loss improved from 0.02276 to 0.02245, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.0225 - val_mse: 0.0225 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0588 - mse: 0.0588\n",
            "Epoch 10: val_loss improved from 0.02245 to 0.02234, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0587 - mse: 0.0587 - val_loss: 0.0223 - val_mse: 0.0223 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0479 - mse: 0.0479\n",
            "Epoch 11: val_loss improved from 0.02234 to 0.02227, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.0223 - val_mse: 0.0223 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0553 - mse: 0.0553\n",
            "Epoch 12: val_loss improved from 0.02227 to 0.02221, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0552 - mse: 0.0552 - val_loss: 0.0222 - val_mse: 0.0222 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0444 - mse: 0.0444\n",
            "Epoch 13: val_loss improved from 0.02221 to 0.02215, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.0221 - val_mse: 0.0221 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0441 - mse: 0.0441\n",
            "Epoch 14: val_loss improved from 0.02215 to 0.02211, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.0221 - val_mse: 0.0221 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0457 - mse: 0.0457\n",
            "Epoch 15: val_loss improved from 0.02211 to 0.02206, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.0221 - val_mse: 0.0221 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0439 - mse: 0.0439\n",
            "Epoch 16: val_loss improved from 0.02206 to 0.02201, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.0220 - val_mse: 0.0220 - learning_rate: 1.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 17: val_loss improved from 0.02201 to 0.02199, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0220 - val_mse: 0.0220 - learning_rate: 1.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 18: val_loss improved from 0.02199 to 0.02197, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0220 - val_mse: 0.0220 - learning_rate: 1.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 19: val_loss improved from 0.02197 to 0.02195, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 20: val_loss improved from 0.02195 to 0.02193, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 21: val_loss improved from 0.02193 to 0.02192, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 22: val_loss improved from 0.02192 to 0.02191, saving model to best_model_fold_2.keras\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 23: val_loss improved from 0.02191 to 0.02191, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 2.0000e-05\n",
            "Epoch 24/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0325 - mse: 0.0325\n",
            "Epoch 24: val_loss improved from 0.02191 to 0.02191, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 2.0000e-05\n",
            "Epoch 25/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 25: val_loss improved from 0.02191 to 0.02191, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 2.0000e-05\n",
            "Epoch 26/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 26: val_loss improved from 0.02191 to 0.02191, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 2.0000e-05\n",
            "Epoch 27/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 27: val_loss improved from 0.02191 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 2.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 28: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 29: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 30: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 31: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 32: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 33: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 34: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 35: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 36: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 37/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0321 - mse: 0.0321\n",
            "Epoch 37: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 38/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 38: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 39/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 39: val_loss improved from 0.02190 to 0.02190, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 40/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0339 - mse: 0.0339\n",
            "Epoch 40: val_loss improved from 0.02190 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 41/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0346 - mse: 0.0346\n",
            "Epoch 41: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 42/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 42: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 43/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 43: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 44/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 44: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 45/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 45: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 46/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 46: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 47/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 47: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 48/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 48: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 49/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 49: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 50/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0318 - mse: 0.0318\n",
            "Epoch 50: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 51/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 51: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 52/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 52: val_loss improved from 0.02189 to 0.02189, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 53/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 53: val_loss improved from 0.02189 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 54/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 54: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 55/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0317 - mse: 0.0317\n",
            "Epoch 55: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 56/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 56: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 57/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 57: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 58/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 58: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 59/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 59: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 60/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 60: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 61/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 61: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 62/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0308 - mse: 0.0308\n",
            "Epoch 62: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 63/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 63: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 64/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 64: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 65/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 65: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 66/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 66: val_loss improved from 0.02188 to 0.02188, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 67/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 67: val_loss improved from 0.02188 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 68/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 68: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 69/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 69: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 70/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 70: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 71/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 71: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 72/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 72: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 73/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 73: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 74/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 74: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 75/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 75: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 76/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 76: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 77/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 77: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 78/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 78: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 79/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 79: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 80/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 80: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 81/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 81: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 82/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 82: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 83/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 83: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 84/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 84: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 85/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 85: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 86/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 86: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 87/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 87: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 88/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 88: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 89/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 89: val_loss improved from 0.02187 to 0.02187, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 90/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 90: val_loss improved from 0.02187 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 91/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 91: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 92/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0325 - mse: 0.0325\n",
            "Epoch 92: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 93/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 93: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 94/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 94: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 95/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 95: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 96/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 96: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 97/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 97: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 98/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 98: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 99/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 99: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 100/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 100: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 101/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 101: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 102/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 102: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 103/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 103: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 104/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 104: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 105/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 105: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 106: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 107: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 108: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 109: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 110: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 111: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 112: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 113: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 114: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 115: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 116: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 117: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 118: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 119: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 120: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 121: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 122: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 123: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 124: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 125: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 126: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 127: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0279 - mse: 0.0279\n",
            "Epoch 128: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 129: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 130: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 131: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 132: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 133: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 134: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 135: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 136: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 137: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 138: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 139: val_loss improved from 0.02186 to 0.02186, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 140: val_loss improved from 0.02186 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 141: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 142: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 143: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 144: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 145: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 146/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 146: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 147/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 147: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 148/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 148: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 149/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 149: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 150/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 150: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 151/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 151: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 152/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 152: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 153/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0261 - mse: 0.0261\n",
            "Epoch 153: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 154/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 154: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 155/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 155: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 156/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 156: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 157/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 157: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 158/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 158: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 159/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0279 - mse: 0.0279\n",
            "Epoch 159: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 160/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 160: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 161/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 161: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 162/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 162: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 163/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 163: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 164/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0279 - mse: 0.0279\n",
            "Epoch 164: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 165/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 165: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 166/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 166: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 167/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0271 - mse: 0.0271\n",
            "Epoch 167: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 168/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 168: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 169/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 169: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 170/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 170: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 171/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 171: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 172/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 172: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 173/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 173: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 174/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0271 - mse: 0.0271\n",
            "Epoch 174: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 175/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 175: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 176/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 176: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 177/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 177: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 178/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 178: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 179/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 179: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 180/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 180: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 181/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 181: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 182/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 182: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 183/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 183: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 184/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 184: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 185/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 185: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 186/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 186: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 187/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 187: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 188/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 188: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 189/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 189: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 190/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 190: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 191/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 191: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 192/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 192: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 193/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 193: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 194/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 194: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 195/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 195: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 196/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 196: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 197/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 197: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 198/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 198: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 199/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 199: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Epoch 200/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 200: val_loss improved from 0.02185 to 0.02185, saving model to best_model_fold_2.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0219 - val_mse: 0.0219 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 200.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5112 - mse: 0.5112\n",
            "Epoch 1: val_loss improved from inf to 0.03062, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.5069 - mse: 0.5069 - val_loss: 0.0306 - val_mse: 0.0306 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3050 - mse: 0.3050\n",
            "Epoch 2: val_loss improved from 0.03062 to 0.02827, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.3046 - mse: 0.3046 - val_loss: 0.0283 - val_mse: 0.0283 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1894 - mse: 0.1894\n",
            "Epoch 3: val_loss improved from 0.02827 to 0.02702, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.1892 - mse: 0.1892 - val_loss: 0.0270 - val_mse: 0.0270 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1544 - mse: 0.1544\n",
            "Epoch 4: val_loss improved from 0.02702 to 0.02640, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.1541 - mse: 0.1541 - val_loss: 0.0264 - val_mse: 0.0264 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1168 - mse: 0.1168\n",
            "Epoch 5: val_loss improved from 0.02640 to 0.02593, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.1168 - mse: 0.1168 - val_loss: 0.0259 - val_mse: 0.0259 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1027 - mse: 0.1027\n",
            "Epoch 6: val_loss improved from 0.02593 to 0.02571, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.1025 - mse: 0.1025 - val_loss: 0.0257 - val_mse: 0.0257 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0965 - mse: 0.0965\n",
            "Epoch 7: val_loss improved from 0.02571 to 0.02552, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0964 - mse: 0.0964 - val_loss: 0.0255 - val_mse: 0.0255 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0749 - mse: 0.0749\n",
            "Epoch 8: val_loss improved from 0.02552 to 0.02541, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0749 - mse: 0.0749 - val_loss: 0.0254 - val_mse: 0.0254 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0755 - mse: 0.0755\n",
            "Epoch 9: val_loss improved from 0.02541 to 0.02526, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0754 - mse: 0.0754 - val_loss: 0.0253 - val_mse: 0.0253 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0647 - mse: 0.0647\n",
            "Epoch 10: val_loss improved from 0.02526 to 0.02514, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0646 - mse: 0.0646 - val_loss: 0.0251 - val_mse: 0.0251 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0682 - mse: 0.0682\n",
            "Epoch 11: val_loss improved from 0.02514 to 0.02504, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0682 - mse: 0.0682 - val_loss: 0.0250 - val_mse: 0.0250 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0594 - mse: 0.0594\n",
            "Epoch 12: val_loss improved from 0.02504 to 0.02495, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0500 - mse: 0.0500\n",
            "Epoch 13: val_loss improved from 0.02495 to 0.02490, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0469 - mse: 0.0469\n",
            "Epoch 14: val_loss improved from 0.02490 to 0.02486, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0508 - mse: 0.0508\n",
            "Epoch 15: val_loss improved from 0.02486 to 0.02483, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0473 - mse: 0.0473\n",
            "Epoch 16: val_loss improved from 0.02483 to 0.02479, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0473 - mse: 0.0473 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0517 - mse: 0.0517\n",
            "Epoch 17: val_loss improved from 0.02479 to 0.02476, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0517 - mse: 0.0517 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 18: val_loss improved from 0.02476 to 0.02474, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 19: val_loss improved from 0.02474 to 0.02472, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 20: val_loss improved from 0.02472 to 0.02470, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 21: val_loss improved from 0.02470 to 0.02467, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 22: val_loss improved from 0.02467 to 0.02465, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0355 - mse: 0.0355\n",
            "Epoch 23: val_loss improved from 0.02465 to 0.02462, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 24: val_loss improved from 0.02462 to 0.02461, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 25: val_loss improved from 0.02461 to 0.02459, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0325 - mse: 0.0325\n",
            "Epoch 26: val_loss improved from 0.02459 to 0.02457, saving model to best_model_fold_3.keras\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 27: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 28: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 29: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0355 - mse: 0.0355\n",
            "Epoch 30: val_loss improved from 0.02457 to 0.02456, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0321 - mse: 0.0321\n",
            "Epoch 31: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 32: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_3.keras\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 33: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 34: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0324 - mse: 0.0324\n",
            "Epoch 35: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0327 - mse: 0.0327\n",
            "Epoch 36: val_loss improved from 0.02456 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 37/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 37: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 38/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 38: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 39/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0322 - mse: 0.0322\n",
            "Epoch 39: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 40/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 40: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 41/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 41: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 42/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 42: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 43/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 43: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 44/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 44: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 45/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 45: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 46/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0321 - mse: 0.0321\n",
            "Epoch 46: val_loss improved from 0.02455 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 47/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 47: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 48/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 48: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 49/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 49: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 50/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 50: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 51/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0325 - mse: 0.0325\n",
            "Epoch 51: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 52/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 52: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 53/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 53: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 54/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 54: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 55/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 55: val_loss improved from 0.02454 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 56/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 56: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 57/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0320 - mse: 0.0320\n",
            "Epoch 57: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 58/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 58: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 59/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0320 - mse: 0.0320\n",
            "Epoch 59: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 60/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 60: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 61/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 61: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 62/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 62: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 63/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 63: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 64/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 64: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 65/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 65: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 66/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 66: val_loss improved from 0.02453 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 67/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 67: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 68/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 68: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 69/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 69: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 70/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 70: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 71/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 71: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 72/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0295 - mse: 0.0295\n",
            "Epoch 72: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 73/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 73: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 74/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 74: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 75/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 75: val_loss improved from 0.02452 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 76/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 76: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 77/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 77: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 78/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 78: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 79/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 79: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 80/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 80: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 81/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 81: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 82/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 82: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 83/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 83: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 84/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0295 - mse: 0.0295\n",
            "Epoch 84: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 85/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 85: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 86/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 86: val_loss improved from 0.02451 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 87/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0317 - mse: 0.0317\n",
            "Epoch 87: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 88/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 88: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 89/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 89: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 90/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 90: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 91/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 91: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 92/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 92: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 93/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0320 - mse: 0.0320\n",
            "Epoch 93: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 94/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 94: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 95/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 95: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 96/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 96: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 97/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 97: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 98/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 98: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 99/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 99: val_loss improved from 0.02450 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 100/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 100: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 101/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0295 - mse: 0.0295\n",
            "Epoch 101: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 102/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 102: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 103/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 103: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 104/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 104: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 105/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 105: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 106: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 107: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 108: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 109: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 110: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 111: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 112: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 113: val_loss improved from 0.02449 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 114: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0317 - mse: 0.0317\n",
            "Epoch 115: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 116: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 117: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 118: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 119: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 120: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 121: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 122: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 123: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 124: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 125: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 126: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 127: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 128: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 129: val_loss improved from 0.02448 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 130: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 131: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 132: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 133: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 134: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 135: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 136: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 137: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 138: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 139: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 140: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 141: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 142: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 143: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 144: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 145: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 146/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 146: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 147/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 147: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 148/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 148: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 149/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 149: val_loss improved from 0.02447 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 150/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 150: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 151/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 151: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 152/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 152: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 153/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 153: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 154/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 154: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 155/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 155: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 156/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 156: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 157/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 157: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 158/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 158: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 159/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 159: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 160/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 160: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 161/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 161: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 162/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 162: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 163/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 163: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 164/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 164: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 165/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 165: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 166/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 166: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 167/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 167: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 168/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 168: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 169/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 169: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 170/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 170: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 171/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 171: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 172/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 172: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 173/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 173: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 174/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 174: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 175/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 175: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 176/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 176: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 177/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 177: val_loss improved from 0.02446 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 178/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 178: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 179/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 179: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 180/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 180: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 181/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 181: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 182/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 182: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 183/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 183: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 184/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - mse: 0.0267\n",
            "Epoch 184: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 185/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 185: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 186/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 186: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 187/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 187: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 188/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 188: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 189/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 189: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 190/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 190: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 191/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 191: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 192/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 192: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 193/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0261 - mse: 0.0261\n",
            "Epoch 193: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 194/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 194: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 195/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 195: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 196/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 196: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 197/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 197: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 198/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 198: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 199/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 199: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Epoch 200/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 200: val_loss improved from 0.02445 to 0.02445, saving model to best_model_fold_3.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 200.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1622 - mse: 0.1622\n",
            "Epoch 1: val_loss improved from inf to 0.03367, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.1607 - mse: 0.1607 - val_loss: 0.0337 - val_mse: 0.0337 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0916 - mse: 0.0916\n",
            "Epoch 2: val_loss improved from 0.03367 to 0.03041, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0915 - mse: 0.0915 - val_loss: 0.0304 - val_mse: 0.0304 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0575 - mse: 0.0575\n",
            "Epoch 3: val_loss improved from 0.03041 to 0.02819, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0575 - mse: 0.0575 - val_loss: 0.0282 - val_mse: 0.0282 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 4: val_loss improved from 0.02819 to 0.02647, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0265 - val_mse: 0.0265 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 5: val_loss improved from 0.02647 to 0.02556, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0256 - val_mse: 0.0256 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 6: val_loss improved from 0.02556 to 0.02515, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0251 - val_mse: 0.0251 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 7: val_loss improved from 0.02515 to 0.02472, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0346 - mse: 0.0346\n",
            "Epoch 8: val_loss improved from 0.02472 to 0.02453, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 9: val_loss improved from 0.02453 to 0.02444, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0244 - val_mse: 0.0244 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 10: val_loss improved from 0.02444 to 0.02435, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.0243 - val_mse: 0.0243 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 11: val_loss improved from 0.02435 to 0.02430, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0243 - val_mse: 0.0243 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0282 - mse: 0.0282\n",
            "Epoch 12: val_loss improved from 0.02430 to 0.02426, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0243 - val_mse: 0.0243 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 13: val_loss improved from 0.02426 to 0.02423, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 14: val_loss improved from 0.02423 to 0.02421, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 15: val_loss improved from 0.02421 to 0.02419, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 16: val_loss improved from 0.02419 to 0.02418, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 17: val_loss improved from 0.02418 to 0.02416, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0279 - mse: 0.0279\n",
            "Epoch 18: val_loss improved from 0.02416 to 0.02416, saving model to best_model_fold_4.keras\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 19: val_loss improved from 0.02416 to 0.02416, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 2.0000e-05\n",
            "Epoch 20/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 20: val_loss improved from 0.02416 to 0.02416, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 2.0000e-05\n",
            "Epoch 21/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 21: val_loss improved from 0.02416 to 0.02416, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 2.0000e-05\n",
            "Epoch 22/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 22: val_loss improved from 0.02416 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 2.0000e-05\n",
            "Epoch 23/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 23: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 2.0000e-05\n",
            "Epoch 24/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 24: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-05\n",
            "Epoch 25/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 25: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-05\n",
            "Epoch 26/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 26: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0242 - val_mse: 0.0242 - learning_rate: 1.0000e-05\n",
            "Epoch 27/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 27: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 28: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 29: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 30: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0270 - mse: 0.0270\n",
            "Epoch 31: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 32: val_loss improved from 0.02415 to 0.02415, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 33: val_loss improved from 0.02415 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 34: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 35: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 36: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 37/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 37: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 38/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 38: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 39/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 39: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 40/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 40: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 41/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0281 - mse: 0.0281\n",
            "Epoch 41: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 42/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0263 - mse: 0.0263\n",
            "Epoch 42: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 43/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 43: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 44/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 44: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 45/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 45: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 46/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 46: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 47/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 47: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 48/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0259 - mse: 0.0259\n",
            "Epoch 48: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 49/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 49: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 50/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 50: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 51/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 51: val_loss improved from 0.02414 to 0.02414, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 52/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - mse: 0.0259\n",
            "Epoch 52: val_loss improved from 0.02414 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 53/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - mse: 0.0259\n",
            "Epoch 53: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 54/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 54: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 55/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 55: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 56/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 56: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 57/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 57: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 58/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 58: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 59/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 59: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 60/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 60: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 61/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 61: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 62/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 62: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 63/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 63: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 64/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0267 - mse: 0.0267\n",
            "Epoch 64: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 65/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 65: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 66/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 66: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 67/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 67: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 68/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 68: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 69/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 69: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 70/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 70: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 71/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 71: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 72/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - mse: 0.0259\n",
            "Epoch 72: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 73/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 73: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 74/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 74: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 75/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0247 - mse: 0.0247\n",
            "Epoch 75: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 76/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 76: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 77/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 77: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 78/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 78: val_loss improved from 0.02413 to 0.02413, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 79/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 79: val_loss improved from 0.02413 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 80/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 80: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 81/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 81: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 82/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 82: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 83/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 83: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 84/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0238 - mse: 0.0238\n",
            "Epoch 84: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 85/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 85: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 86/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 86: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 87/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 87: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 88/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 88: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 89/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 89: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 90/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 90: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 91/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 91: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 92/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 92: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 93/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0259 - mse: 0.0259\n",
            "Epoch 93: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 94/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 94: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 95/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0242 - mse: 0.0242\n",
            "Epoch 95: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 96/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 96: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 97/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 97: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 98/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 98: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 99/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 99: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 100/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 100: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 101/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0265 - mse: 0.0265\n",
            "Epoch 101: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 102/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 102: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 103/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0267 - mse: 0.0267\n",
            "Epoch 103: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 104/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0242 - mse: 0.0242\n",
            "Epoch 104: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 105/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 105: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0247 - mse: 0.0247\n",
            "Epoch 106: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 107: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 108: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 109: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 110: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0257 - mse: 0.0257\n",
            "Epoch 111: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 112: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 113: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 114: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 115: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0267 - mse: 0.0267\n",
            "Epoch 116: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0236 - mse: 0.0236\n",
            "Epoch 117: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 118: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 119: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 120: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0243 - mse: 0.0243\n",
            "Epoch 121: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 122: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 123: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 124: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 125: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 126: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 127: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 128: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 129: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 130: val_loss improved from 0.02412 to 0.02412, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 131: val_loss improved from 0.02412 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 132: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 133: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 134: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0243 - mse: 0.0243\n",
            "Epoch 135: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 136: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 137: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 138: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 139: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 140: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 141: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 142: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 143: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 144: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0237 - mse: 0.0237\n",
            "Epoch 145: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 146/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0247 - mse: 0.0247\n",
            "Epoch 146: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 147/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 147: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 148/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 148: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 149/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 149: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 150/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 150: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 151/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0242 - mse: 0.0242\n",
            "Epoch 151: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 152/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 152: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 153/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0243 - mse: 0.0243\n",
            "Epoch 153: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 154/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0255 - mse: 0.0255\n",
            "Epoch 154: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 155/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0240 - mse: 0.0240\n",
            "Epoch 155: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 156/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 156: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 157/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 157: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 158/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 158: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 159/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 159: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 160/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 160: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 161/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 161: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 162/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0247 - mse: 0.0247\n",
            "Epoch 162: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 163/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 163: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 164/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 164: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 165/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0258 - mse: 0.0258\n",
            "Epoch 165: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 166/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0256 - mse: 0.0256\n",
            "Epoch 166: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 167/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 167: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 168/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 168: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 169/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 169: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 170/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 170: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 171/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0238 - mse: 0.0238\n",
            "Epoch 171: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 172/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 172: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 173/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 173: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 174/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0238 - mse: 0.0238\n",
            "Epoch 174: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0239 - mse: 0.0239 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 175/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 175: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 176/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0242 - mse: 0.0242\n",
            "Epoch 176: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 177/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 177: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 178/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 178: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 179/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0247 - mse: 0.0247\n",
            "Epoch 179: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 180/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 180: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 181/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0240 - mse: 0.0240\n",
            "Epoch 181: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 182/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 182: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 183/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 183: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 184/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0245 - mse: 0.0245\n",
            "Epoch 184: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 185/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 185: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 186/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0248 - mse: 0.0248\n",
            "Epoch 186: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 187/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 187: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 188/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 188: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 189/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 189: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 190/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 190: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 191/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0241 - mse: 0.0241\n",
            "Epoch 191: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 192/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 192: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 193/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0250 - mse: 0.0250\n",
            "Epoch 193: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 194/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0249 - mse: 0.0249\n",
            "Epoch 194: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 195/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0242 - mse: 0.0242\n",
            "Epoch 195: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 196/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0243 - mse: 0.0243\n",
            "Epoch 196: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 197/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0244 - mse: 0.0244\n",
            "Epoch 197: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 198/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0251 - mse: 0.0251\n",
            "Epoch 198: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 199/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0246 - mse: 0.0246\n",
            "Epoch 199: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Epoch 200/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0252 - mse: 0.0252\n",
            "Epoch 200: val_loss improved from 0.02411 to 0.02411, saving model to best_model_fold_4.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0241 - val_mse: 0.0241 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 200.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2605 - mse: 0.2605\n",
            "Epoch 1: val_loss improved from inf to 0.03036, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.0304 - val_mse: 0.0304 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1588 - mse: 0.1588\n",
            "Epoch 2: val_loss improved from 0.03036 to 0.02623, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.1580 - mse: 0.1580 - val_loss: 0.0262 - val_mse: 0.0262 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1109 - mse: 0.1109\n",
            "Epoch 3: val_loss improved from 0.02623 to 0.02543, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1107 - mse: 0.1107 - val_loss: 0.0254 - val_mse: 0.0254 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0930 - mse: 0.0930\n",
            "Epoch 4: val_loss improved from 0.02543 to 0.02510, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.0251 - val_mse: 0.0251 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0772 - mse: 0.0772\n",
            "Epoch 5: val_loss improved from 0.02510 to 0.02497, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0771 - mse: 0.0771 - val_loss: 0.0250 - val_mse: 0.0250 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0653 - mse: 0.0653\n",
            "Epoch 6: val_loss improved from 0.02497 to 0.02490, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.0249 - val_mse: 0.0249 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0514 - mse: 0.0514\n",
            "Epoch 7: val_loss improved from 0.02490 to 0.02481, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0515 - mse: 0.0515 - val_loss: 0.0248 - val_mse: 0.0248 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0567 - mse: 0.0567\n",
            "Epoch 8: val_loss improved from 0.02481 to 0.02475, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0566 - mse: 0.0566 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0476 - mse: 0.0476\n",
            "Epoch 9: val_loss improved from 0.02475 to 0.02473, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0478 - mse: 0.0478 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0504 - mse: 0.0504\n",
            "Epoch 10: val_loss improved from 0.02473 to 0.02470, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0497 - mse: 0.0497\n",
            "Epoch 11: val_loss improved from 0.02470 to 0.02466, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0497 - mse: 0.0497 - val_loss: 0.0247 - val_mse: 0.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0481 - mse: 0.0481\n",
            "Epoch 12: val_loss improved from 0.02466 to 0.02464, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0463 - mse: 0.0463\n",
            "Epoch 13: val_loss improved from 0.02464 to 0.02462, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0462 - mse: 0.0462 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 14: val_loss improved from 0.02462 to 0.02460, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 15: val_loss improved from 0.02460 to 0.02460, saving model to best_model_fold_5.keras\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 16: val_loss improved from 0.02460 to 0.02460, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 17/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 17: val_loss improved from 0.02460 to 0.02459, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 18/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 18: val_loss improved from 0.02459 to 0.02459, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 19/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 19: val_loss improved from 0.02459 to 0.02459, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 20/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0448 - mse: 0.0448\n",
            "Epoch 20: val_loss improved from 0.02459 to 0.02459, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 21/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 21: val_loss improved from 0.02459 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 2.0000e-05\n",
            "Epoch 22/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 22: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 23/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 23: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 24/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 24: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 25/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 25: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 26/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0457 - mse: 0.0457\n",
            "Epoch 26: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 27/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0342 - mse: 0.0342\n",
            "Epoch 27: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 28: val_loss improved from 0.02458 to 0.02458, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 29: val_loss improved from 0.02458 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 30: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 31: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 32: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 33: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 34: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 35: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0326 - mse: 0.0326\n",
            "Epoch 36: val_loss improved from 0.02457 to 0.02457, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 37/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 37: val_loss improved from 0.02457 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 38/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 38: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 39/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 39: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 40/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0354 - mse: 0.0354\n",
            "Epoch 40: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 41/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 41: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 42/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 42: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 43/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 43: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 44/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 44: val_loss improved from 0.02456 to 0.02456, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 45/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0340 - mse: 0.0340\n",
            "Epoch 45: val_loss improved from 0.02456 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 46/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 46: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 47/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 47: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 48/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 48: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 49/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 49: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0246 - val_mse: 0.0246 - learning_rate: 1.0000e-05\n",
            "Epoch 50/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 50: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0316 - mse: 0.0316 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 51/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 51: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 52/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 52: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 53/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0354 - mse: 0.0354\n",
            "Epoch 53: val_loss improved from 0.02455 to 0.02455, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 54/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0328 - mse: 0.0328\n",
            "Epoch 54: val_loss improved from 0.02455 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 55/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 55: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 56/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 56: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 57/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0345 - mse: 0.0345\n",
            "Epoch 57: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 58/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 58: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 59/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 59: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 60/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 60: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 61/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 61: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 62/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 62: val_loss improved from 0.02454 to 0.02454, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 63/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 63: val_loss improved from 0.02454 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 64/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 64: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 65/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0354 - mse: 0.0354\n",
            "Epoch 65: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 66/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 66: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 67/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 67: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 68/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 68: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 69/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 69: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 70/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 70: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 71/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 71: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 72/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 72: val_loss improved from 0.02453 to 0.02453, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 73/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0340 - mse: 0.0340\n",
            "Epoch 73: val_loss improved from 0.02453 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 74/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 74: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 75/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0342 - mse: 0.0342\n",
            "Epoch 75: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 76/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 76: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 77/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0308 - mse: 0.0308\n",
            "Epoch 77: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 78/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 78: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 79/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 79: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 80/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 80: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 81/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0326 - mse: 0.0326\n",
            "Epoch 81: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 82/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 82: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 83/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 83: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 84/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0308 - mse: 0.0308\n",
            "Epoch 84: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 85/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0318 - mse: 0.0318\n",
            "Epoch 85: val_loss improved from 0.02452 to 0.02452, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 86/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 86: val_loss improved from 0.02452 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 87/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 87: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 88/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 88: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 89/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 89: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 90/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 90: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 91/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 91: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 92/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 92: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 93/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 93: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 94/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 94: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 95/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 95: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 96/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 96: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 97/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0285 - mse: 0.0285\n",
            "Epoch 97: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 98/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 98: val_loss improved from 0.02451 to 0.02451, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 99/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 99: val_loss improved from 0.02451 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 100/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0348 - mse: 0.0348\n",
            "Epoch 100: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 101/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 101: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 102/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 102: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 103/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0309 - mse: 0.0309\n",
            "Epoch 103: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 104/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 104: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 105/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 105: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0318 - mse: 0.0318\n",
            "Epoch 106: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 107: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0345 - mse: 0.0345\n",
            "Epoch 108: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0318 - mse: 0.0318\n",
            "Epoch 109: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 110: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 111: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 112: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 113: val_loss improved from 0.02450 to 0.02450, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 114: val_loss improved from 0.02450 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 115: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 116: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 117: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 118: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0275 - mse: 0.0275\n",
            "Epoch 119: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 120: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 121: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 122: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 123: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 124: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 125: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 126: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 127: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0314 - mse: 0.0314\n",
            "Epoch 128: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 129: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 130: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0291 - mse: 0.0291\n",
            "Epoch 131: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 132: val_loss improved from 0.02449 to 0.02449, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 133: val_loss improved from 0.02449 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0308 - mse: 0.0308\n",
            "Epoch 134: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 135: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 136: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 137: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 138: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 139: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0317 - mse: 0.0317\n",
            "Epoch 140: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0269 - mse: 0.0269\n",
            "Epoch 141: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0269 - mse: 0.0269 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0315 - mse: 0.0315\n",
            "Epoch 142: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0327 - mse: 0.0327\n",
            "Epoch 143: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 144: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 145: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 146/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 146: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 147/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 147: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 148/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 148: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 149/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 149: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 150/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 150: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 151/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0327 - mse: 0.0327\n",
            "Epoch 151: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 152/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0296 - mse: 0.0296\n",
            "Epoch 152: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 153/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 153: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 154/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0310 - mse: 0.0310\n",
            "Epoch 154: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 155/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 155: val_loss improved from 0.02448 to 0.02448, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 156/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 156: val_loss improved from 0.02448 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 157/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 157: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 158/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 158: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 159/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0324 - mse: 0.0324\n",
            "Epoch 159: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 160/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 160: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 161/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0302 - mse: 0.0302\n",
            "Epoch 161: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 162/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0278 - mse: 0.0278\n",
            "Epoch 162: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 163/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 163: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 164/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0280 - mse: 0.0280\n",
            "Epoch 164: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 165/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0317 - mse: 0.0317\n",
            "Epoch 165: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 166/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 166: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 167/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 167: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 168/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 168: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 169/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 169: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 170/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 170: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 171/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0306 - mse: 0.0306\n",
            "Epoch 171: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 172/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 172: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 173/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0325 - mse: 0.0325\n",
            "Epoch 173: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 174/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 174: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 175/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0297 - mse: 0.0297\n",
            "Epoch 175: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0297 - mse: 0.0297 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 176/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0308 - mse: 0.0308\n",
            "Epoch 176: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 177/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 177: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 178/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 178: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 179/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0277 - mse: 0.0277\n",
            "Epoch 179: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 180/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0304 - mse: 0.0304\n",
            "Epoch 180: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 181/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0294 - mse: 0.0294\n",
            "Epoch 181: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 182/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0316 - mse: 0.0316\n",
            "Epoch 182: val_loss improved from 0.02447 to 0.02447, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 183/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 183: val_loss improved from 0.02447 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 184/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0254 - mse: 0.0254\n",
            "Epoch 184: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 185/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0266 - mse: 0.0266\n",
            "Epoch 185: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 186/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0272 - mse: 0.0272\n",
            "Epoch 186: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 187/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0295 - mse: 0.0295\n",
            "Epoch 187: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 188/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 188: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 189/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 189: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 190/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0262 - mse: 0.0262\n",
            "Epoch 190: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 191/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0288 - mse: 0.0288\n",
            "Epoch 191: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 192/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 192: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 193/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - mse: 0.0289\n",
            "Epoch 193: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 194/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0271 - mse: 0.0271\n",
            "Epoch 194: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 195/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0274 - mse: 0.0274\n",
            "Epoch 195: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 196/200\n",
            "\u001b[1m50/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 196: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 197/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0307 - mse: 0.0307\n",
            "Epoch 197: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 198/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0300 - mse: 0.0300\n",
            "Epoch 198: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 199/200\n",
            "\u001b[1m49/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0259 - mse: 0.0259\n",
            "Epoch 199: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Epoch 200/200\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0260 - mse: 0.0260\n",
            "Epoch 200: val_loss improved from 0.02446 to 0.02446, saving model to best_model_fold_5.keras\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0245 - val_mse: 0.0245 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 200.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABe+klEQVR4nO3dd3hT9f4H8HeSNklnugeldFF2KVhoKSBDKlNlKiI/GYKoDOUiXsXB1FuvA1Hk4rgKOBCE60SZFVChbMoGy2yhm9I90iTf3x+lgdhSCj3JoeX9ep48tCcn53xOTmne/Y5zFEIIASIiIqJGQil3AURERERSYrghIiKiRoXhhoiIiBoVhhsiIiJqVBhuiIiIqFFhuCEiIqJGheGGiIiIGhWGGyIiImpUGG6IiIioUWG4ISJJBAcHY9y4cbf1WoVCgblz50paj5ySk5PRt29f6HQ6KBQK/PDDD3KXVCfnz5+HQqHA8uXLb7ruuHHjEBwcbPWaiG4Hww1RHSxfvhwKhcLi4ePjg969e2P9+vVW229JSQnmzp2Lbdu21Wn9bdu2mev76quvalynW7duUCgUaNeunYSVWl/VB+8777wjdyk3NXbsWBw5cgRvvPEGvvzyS3Tq1Mlq+6p6X2p6dOnSxWr7rQuTyYTly5fjoYceQmBgIJycnNCuXTu8/vrrKCsrk7U2atzs5C6AqCGZP38+QkJCIIRAZmYmli9fjoEDB+Lnn3/GAw88IPn+SkpKMG/ePABAr1696vw6rVaLlStX4v/+7/8slp8/fx47d+6EVquVsky6TmlpKRITE/HKK69g6tSpNtvvqFGjMHDgQItl3t7eNtt/TUpKSjB+/Hh06dIFTz/9NHx8fJCYmIg5c+YgISEBv/32GxQKhaw1UuPEcEN0CwYMGGDxV/iECRPg6+uLb775xirh5nYNHDgQP/30E3JycuDl5WVevnLlSvj6+iI8PBxXrlyRscLGKzs7GwDg5uYm2TaLi4vh5ORU6zr33HNPtTArN7VajR07dqBr167mZU8++SSCg4PNAScuLk7GCqmxYrcUUT24ubnBwcEBdnaWfyeYTCYsWrQIbdu2hVarha+vL5566qlqgWLfvn3o168fvLy84ODggJCQEDzxxBMAKltZqv7ynjdvnrmroS5jUwYPHgyNRoM1a9ZYLF+5ciUeeeQRqFSqaq8xGAxYsGABwsLCoNFoEBwcjJdffhnl5eUW6wkh8Prrr6Np06ZwdHRE7969cezYsRrryMvLw/Tp0xEYGAiNRoPmzZvj3//+N0wm002P4XZlZWWZQ6dWq0VkZCRWrFhRbb1Vq1YhKioKLi4ucHV1RUREBN5//33z8xUVFZg3bx7Cw8Oh1Wrh6emJ7t27Y/PmzTfc99y5cxEUFAQAeOGFF6BQKCzGpRw8eBADBgyAq6srnJ2d0adPH+zatctiG1VdoNu3b8fkyZPh4+ODpk2b1vNdAc6ePYuHH34YHh4ecHR0RJcuXfDLL7/U6bU//PAD2rVrB61Wi3bt2uH777+v0+vUarVFsKkydOhQAMCJEyfqfgBEt4AtN0S3ID8/Hzk5ORBCICsrC4sXL0ZRUVG1v5ifeuopLF++HOPHj8ezzz6Lc+fO4cMPP8TBgwexY8cO2NvbIysrC3379oW3tzdeeukluLm54fz58/juu+8AVHYpLF26FM888wyGDh2KYcOGAQDat29/0zodHR0xePBgfPPNN3jmmWcAAIcOHcKxY8fw3//+F4cPH672mokTJ2LFihUYMWIEnn/+eezevRvx8fE4ceKExYfZ7Nmz8frrr2PgwIEYOHAgDhw4gL59+0Kv11tsr6SkBD179sSlS5fw1FNPoVmzZti5cydmzZqF9PR0LFq06Jbe+7ooLS1Fr169cPr0aUydOhUhISFYs2YNxo0bh7y8PDz33HMAgM2bN2PUqFHo06cP/v3vfwOo/KDdsWOHeZ25c+ciPj4eEydORHR0NAoKCrBv3z4cOHAA999/f437HzZsGNzc3PCPf/zD3E3k7OwMADh27BjuvfdeuLq64p///Cfs7e3x8ccfo1evXti+fTtiYmIstjV58mR4e3tj9uzZKC4uvumxl5SUICcnx2KZTqeDvb09MjMz0bVrV5SUlODZZ5+Fp6cnVqxYgYceeghr1641h42abNq0CcOHD0ebNm0QHx+Py5cvY/z48fUKXBkZGQBg0apIJClBRDe1bNkyAaDaQ6PRiOXLl1us+8cffwgA4uuvv7ZYvmHDBovl33//vQAg9u7de8P9ZmdnCwBizpw5dapz69atAoBYs2aNWLdunVAoFCIlJUUIIcQLL7wgQkNDhRBC9OzZU7Rt29b8uqSkJAFATJw40WJ7M2fOFADEb7/9JoQQIisrS6jVajFo0CBhMpnM67388ssCgBg7dqx52YIFC4STk5P466+/LLb50ksvCZVKZa5LCFGnYzx37pwAIN5+++0brrNo0SIBQHz11VfmZXq9XsTGxgpnZ2dRUFAghBDiueeeE66ursJgMNxwW5GRkWLQoEG11nQrdQ4ZMkSo1Wpx5swZ87K0tDTh4uIievToYV5W9bPWvXv3Wuv7+/5qemzdulUIIcT06dMFAPHHH3+YX1dYWChCQkJEcHCwMBqNFttatmyZeb0OHToIf39/kZeXZ162adMmAUAEBQXdyltjFhcXJ1xdXcWVK1du6/VEN8NuKaJbsGTJEmzevBmbN2/GV199hd69e2PixInm1hYAWLNmDXQ6He6//37k5OSYH1FRUXB2dsbWrVsBXBuTsW7dOlRUVEhea9++feHh4YFVq1ZBCIFVq1Zh1KhRNa7766+/AgBmzJhhsfz5558HAHP3xZYtW6DX6zFt2jSLgaDTp0+vts01a9bg3nvvhbu7u8X7EBcXB6PRiN9//12Kw6x2HH5+fhbHaW9vj2effRZFRUXYvn07gMr3vri4uNYuJjc3Nxw7dgzJycn1rstoNGLTpk0YMmQIQkNDzcv9/f3x2GOP4c8//0RBQYHFa5588skauw9vZNKkSeafzapHZGQkgMr3JTo6Gt27dzev7+zsjEmTJuH8+fM4fvx4jdtMT09HUlISxo4dC51OZ15+//33o02bNnWu7Xr/+te/sGXLFrz55puSjksiuh67pYhuQXR0tMWA4lGjRqFjx46YOnUqHnjgAajVaiQnJyM/Px8+Pj41biMrKwsA0LNnTwwfPhzz5s3De++9h169emHIkCF47LHHoNFo6l2rvb09Hn74YaxcuRLR0dFITU3FY489VuO6Fy5cgFKpRPPmzS2W+/n5wc3NDRcuXDCvBwDh4eEW63l7e8Pd3d1iWXJyMg4fPnzDGTtV74OULly4gPDwcCiVln+3tW7d2vw8UNnl8+2332LAgAEICAhA37598cgjj6B///7m18yfPx+DBw9GixYt0K5dO/Tv3x+PP/54nboF/y47OxslJSVo2bJltedat24Nk8mE1NRUtG3b1rw8JCTklvYRHh5+w8G5Fy5cqNbtVbXvqudrujTAjc43ALRs2RIHDhy4pRpXr16NV199FRMmTDB3lxJZA8MNUT0olUr07t0b77//PpKTk9G2bVuYTCb4+Pjg66+/rvE1VR/2CoUCa9euxa5du/Dzzz9j48aNeOKJJ/Duu+9i165d5rEa9fHYY4/ho48+wty5cxEZGXnTv7alnJZrMplw//3345///GeNz7do0UKyfd0qHx8fJCUlYePGjVi/fj3Wr1+PZcuWYcyYMebBxz169MCZM2fw448/YtOmTfjvf/+L9957Dx999BEmTpxo9RodHBysvg9b2rx5M8aMGYNBgwbho48+krscauQYbojqyWAwAACKiooAAGFhYdiyZQu6detWpw+oLl26oEuXLnjjjTewcuVKjB49GqtWrcLEiRPrHTa6d++OZs2aYdu2beaBszUJCgqCyWRCcnKy+a95AMjMzEReXp55BlDVv8nJyRbdK9nZ2dVmgoWFhaGoqMimU32DgoJw+PBhmEwmi9abkydPmp+volar8eCDD+LBBx+EyWTC5MmT8fHHH+O1114zt2B5eHhg/PjxGD9+PIqKitCjRw/MnTv3lsONt7c3HB0dcerUqWrPnTx5EkqlEoGBgbdzyHUSFBR0w31XPX+j1wGosWuupu3dyO7duzF06FB06tQJ3377bbXZhURS45gbonqoqKjApk2boFarzaHgkUcegdFoxIIFC6qtbzAYkJeXBwC4cuUKhBAWz3fo0AEAzNOvHR0dAcD8mlulUCjwwQcfYM6cOXj88cdvuF7Vxd/+PoNp4cKFAIBBgwYBAOLi4mBvb4/Fixdb1F7TzKdHHnkEiYmJ2LhxY7Xn8vLyzKFQSgMHDkRGRgZWr15tXmYwGLB48WI4OzujZ8+eAIDLly9bvE6pVJq7m6re+7+v4+zsjObNm1ebGl8XKpUKffv2xY8//ojz58+bl2dmZmLlypXo3r07XF1db3m7dTVw4EDs2bMHiYmJ5mXFxcX45JNPEBwcfMMWPX9/f3To0AErVqxAfn6+efnmzZtvOE7n706cOIFBgwYhODgY69ata3QtUnRnYnwmugXr1683/7WblZWFlStXIjk5GS+99JL5w6lnz5546qmnEB8fj6SkJPTt2xf29vZITk7GmjVr8P7772PEiBFYsWIF/vOf/2Do0KEICwtDYWEhPv30U7i6uprDhoODA9q0aYPVq1ejRYsW8PDwQLt27W7p1gmDBw/G4MGDa10nMjISY8eOxSeffIK8vDz07NkTe/bswYoVKzBkyBD07t0bQGULxMyZMxEfH48HHngAAwcOxMGDB7F+/fpq03pfeOEF/PTTT3jggQcwbtw4REVFobi4GEeOHMHatWtx/vz525oKnJCQUOOl+4cMGYJJkybh448/xrhx47B//34EBwdj7dq12LFjBxYtWgQXFxcAldPec3Nzcd9996Fp06a4cOECFi9ejA4dOphDaps2bdCrVy9ERUXBw8MD+/btw9q1a2/7qsOvv/46Nm/ejO7du2Py5Mmws7PDxx9/jPLycrz11lu3tc26eumll/DNN99gwIABePbZZ+Hh4YEVK1bg3Llz+N///ldtjNL14uPjMWjQIHTv3h1PPPEEcnNzsXjxYrRt29bcWnkjhYWF6NevH65cuYIXXnih2nV1wsLCEBsbK8kxElmQebYWUYNQ01RwrVYrOnToIJYuXWoxLbrKJ598IqKiooSDg4NwcXERERER4p///KdIS0sTQghx4MABMWrUKNGsWTOh0WiEj4+PeOCBB8S+ffsstrNz504RFRUl1Gr1TadMXz8VvDZ/nwouhBAVFRVi3rx5IiQkRNjb24vAwEAxa9YsUVZWZrGe0WgU8+bNE/7+/sLBwUH06tVLHD16VAQFBVlMBReicrrxrFmzRPPmzYVarRZeXl6ia9eu4p133hF6vd683s2OS4japzwDEF9++aUQQojMzEwxfvx44eXlJdRqtYiIiLCY2iyEEGvXrhV9+/YVPj4+Qq1Wi2bNmomnnnpKpKenm9d5/fXXRXR0tHBzcxMODg6iVatW4o033rCou7Y6a5qyfuDAAdGvXz/h7OwsHB0dRe/evcXOnTst1qn6WavtEgF13d/1zpw5I0aMGCHc3NyEVqsV0dHRYt26dTVu6+/v1//+9z/RunVrodFoRJs2bcR3330nxo4de9Op4Dc7Z3//eSGSikKIv7WLExERETVgHHNDREREjQrDDRERETUqDDdERETUqDDcEBERUaPCcENERESNCsMNERERNSp33UX8TCYT0tLS4OLiIul9dIiIiMh6hBAoLCxEkyZNar3wJHAXhpu0tDSr3sOFiIiIrCc1NRVNmzatdZ27LtxUXX49NTXVqvdyISIiIukUFBQgMDDQ/Dlem7su3FR1Rbm6ujLcEBERNTB1GVLCAcVERETUqDDcEBERUaPCcENERESNyl035oaIiBoXo9GIiooKucsgCajV6ptO864LhhsiImqQhBDIyMhAXl6e3KWQRJRKJUJCQqBWq+u1HYYbIiJqkKqCjY+PDxwdHXlh1gau6iK76enpaNasWb3OJ8MNERE1OEaj0RxsPD095S6HJOLt7Y20tDQYDAbY29vf9nY4oJiIiBqcqjE2jo6OMldCUqrqjjIajfXaDsMNERE1WOyKalykOp8MN0RERNSoMNwQERE1cMHBwVi0aJHcZdwxGG6IiIhsRKFQ1PqYO3fubW137969mDRpUr1q69WrF6ZPn16vbdwpOFtKIuUGI3KK9FAAaOLmIHc5RER0B0pPTzd/vXr1asyePRunTp0yL3N2djZ/LYSA0WiEnd3NP6q9vb2lLbSBY8uNRI5eyke3N3/DY5/ukrsUIiK6Q/n5+ZkfOp0OCoXC/P3Jkyfh4uKC9evXIyoqChqNBn/++SfOnDmDwYMHw9fXF87OzujcuTO2bNlisd2/d0spFAr897//xdChQ+Ho6Ijw8HD89NNP9ar9f//7H9q2bQuNRoPg4GC8++67Fs//5z//QXh4OLRaLXx9fTFixAjzc2vXrkVERAQcHBzg6emJuLg4FBcX16ue2rDlRiKqq5eLNpiEzJUQEd2dhBAorajfFOLb5WCvkmymz0svvYR33nkHoaGhcHd3R2pqKgYOHIg33ngDGo0GX3zxBR588EGcOnUKzZo1u+F25s2bh7feegtvv/02Fi9ejNGjR+PChQvw8PC45Zr279+PRx55BHPnzsXIkSOxc+dOTJ48GZ6enhg3bhz27duHZ599Fl9++SW6du2K3Nxc/PHHHwAqW6tGjRqFt956C0OHDkVhYSH++OMPCGG9z0uGG4morv5QGxluiIhkUVphRJvZG2XZ9/H5/eColuYjdf78+bj//vvN33t4eCAyMtL8/YIFC/D999/jp59+wtSpU2+4nXHjxmHUqFEAgH/961/44IMPsGfPHvTv3/+Wa1q4cCH69OmD1157DQDQokULHD9+HG+//TbGjRuHlJQUODk54YEHHoCLiwuCgoLQsWNHAJXhxmAwYNiwYQgKCgIARERE3HINt4LdUhJRKSvDDVtuiIioPjp16mTxfVFREWbOnInWrVvDzc0Nzs7OOHHiBFJSUmrdTvv27c1fOzk5wdXVFVlZWbdV04kTJ9CtWzeLZd26dUNycjKMRiPuv/9+BAUFITQ0FI8//ji+/vprlJSUAAAiIyPRp08fRERE4OGHH8ann36KK1eu3FYddcWWG4nYqSrDjYnhhohIFg72Khyf30+2fUvFycnJ4vuZM2di8+bNeOedd9C8eXM4ODhgxIgR0Ov1tW7n77cvUCgUMJlMktV5PRcXFxw4cADbtm3Dpk2bMHv2bMydOxd79+6Fm5sbNm/ejJ07d2LTpk1YvHgxXnnlFezevRshISFWqYfhRiJsuSEikpdCoZCsa+hOsmPHDowbNw5Dhw4FUNmSc/78eZvW0Lp1a+zYsaNaXS1atIBKVRns7OzsEBcXh7i4OMyZMwdubm747bffMGzYMCgUCnTr1g3dunXD7NmzERQUhO+//x4zZsywSr2N76dAJnZKjrkhIiLphYeH47vvvsODDz4IhUKB1157zWotMNnZ2UhKSrJY5u/vj+effx6dO3fGggULMHLkSCQmJuLDDz/Ef/7zHwDAunXrcPbsWfTo0QPu7u749ddfYTKZ0LJlS+zevRsJCQno27cvfHx8sHv3bmRnZ6N169ZWOQaA4UYy11purPMDR0REd6eFCxfiiSeeQNeuXeHl5YUXX3wRBQUFVtnXypUrsXLlSotlCxYswKuvvopvv/0Ws2fPxoIFC+Dv74/58+dj3LhxAAA3Nzd89913mDt3LsrKyhAeHo5vvvkGbdu2xYkTJ/D7779j0aJFKCgoQFBQEN59910MGDDAKscAAAphzblYd6CCggLodDrk5+fD1dVVsu1m5JehS3wC7FUKJL8xULLtEhFRdWVlZTh37hxCQkKg1WrlLockUtt5vZXPb86WkgjH3BAREd0ZGG4kUhVuhOCMKSIiIjkx3EikKtwAbL0hIiKSE8ONROyuCzemu2sYExER0R2F4UYibLkhIiK6MzDcSOT6lhujkeGGiIhILgw3ErFsueG1boiIiOTCcCMRhUJhDji8SjEREZF8GG4kpFLwWjdERERyY7iREFtuiIjIFnr16oXp06fLXcYdi+FGQrx5JhER1ebBBx9E//79a3zujz/+gEKhwOHDh+u9n+XLl8PNza3e22moGG4kpFKxW4qIiG5swoQJ2Lx5My5evFjtuWXLlqFTp05o3769DJU1Lgw3EmLLDRER1eaBBx6At7c3li9fbrG8qKgIa9aswYQJE3D58mWMGjUKAQEBcHR0REREBL755htJ60hJScHgwYPh7OwMV1dXPPLII8jMzDQ/f+jQIfTu3RsuLi5wdXVFVFQU9u3bBwC4cOECHnzwQbi7u8PJyQlt27bFr7/+Kml99WUndwGNybWbZ3IqOBGRzQkBVJTIs297R0ChuOlqdnZ2GDNmDJYvX45XXnkFiquvWbNmDYxGI0aNGoWioiJERUXhxRdfhKurK3755Rc8/vjjCAsLQ3R0dL1LNZlM5mCzfft2GAwGTJkyBSNHjsS2bdsAAKNHj0bHjh2xdOlSqFQqJCUlwd7eHgAwZcoU6PV6/P7773BycsLx48fh7Oxc77qkxHAjITtlZUMYW26IiGRQUQL8q4k8+345DVA71WnVJ554Am+//Ta2b9+OXr16Aajskho+fDh0Oh10Oh1mzpxpXn/atGnYuHEjvv32W0nCTUJCAo4cOYJz584hMDAQAPDFF1+gbdu22Lt3Lzp37oyUlBS88MILaNWqFQAgPDzc/PqUlBQMHz4cERERAIDQ0NB61yQ1dktJ6Gq24ZgbIiK6oVatWqFr1674/PPPAQCnT5/GH3/8gQkTJgAAjEYjFixYgIiICHh4eMDZ2RkbN25ESkqKJPs/ceIEAgMDzcEGANq0aQM3NzecOHECADBjxgxMnDgRcXFxePPNN3HmzBnzus8++yxef/11dOvWDXPmzJFkALTU2HIjIbbcEBHJyN6xsgVFrn3fggkTJmDatGlYsmQJli1bhrCwMPTs2RMA8Pbbb+P999/HokWLEBERAScnJ0yfPh16vd4alddo7ty5eOyxx/DLL79g/fr1mDNnDlatWoWhQ4di4sSJ6NevH3755Rds2rQJ8fHxePfddzFt2jSb1XczbLmREK9zQ0QkI4WismtIjkcdxttc75FHHoFSqcTKlSvxxRdf4IknnjCPv9mxYwcGDx6M//u//0NkZCRCQ0Px119/SfY2tW7dGqmpqUhNTTUvO378OPLy8tCmTRvzshYtWuAf//gHNm3ahGHDhmHZsmXm5wIDA/H000/ju+++w/PPP49PP/1UsvqkwJYbCXG2FBER1YWzszNGjhyJWbNmoaCgAOPGjTM/Fx4ejrVr12Lnzp1wd3fHwoULkZmZaRE86sJoNCIpKclimUajQVxcHCIiIjB69GgsWrQIBoMBkydPRs+ePdGpUyeUlpbihRdewIgRIxASEoKLFy9i7969GD58OABg+vTpGDBgAFq0aIErV65g69ataN26dX3fEkkx3Ejo2mwphhsiIqrdhAkT8Nlnn2HgwIFo0uTaQOhXX30VZ8+eRb9+/eDo6IhJkyZhyJAhyM/Pv6XtFxUVoWPHjhbLwsLCcPr0afz444+YNm0aevToAaVSif79+2Px4sUAAJVKhcuXL2PMmDHIzMyEl5cXhg0bhnnz5gGoDE1TpkzBxYsX4erqiv79++O9996r57shLYUQ4q76JC4oKIBOp0N+fj5cXV0l3fbgD//EoYv5+HxcJ9zXylfSbRMR0TVlZWU4d+4cQkJCoNVq5S6HJFLbeb2Vz2+OuZGQueXGeFflRSIiojsKw42EOFuKiIhIfgw3EuJ1boiIiOTHcCMhttwQERHJj+FGQrzODRGRbd1lc2IaPanOJ8ONhHidGyIi26i6iWNJiUw3yiSrqLoKs0qlqtd2eJ0bCfE6N0REtqFSqeDm5oasrCwAgKOjo/kKv9QwmUwmZGdnw9HREXZ29YsnDDcSslNVtdyYZK6EiKjx8/PzAwBzwKGGT6lUolmzZvUOqgw3ElJdHVDMlhsiIutTKBTw9/eHj48PKioq5C6HJKBWq6FU1n/EDMONhK423HDMDRGRDalUqnqP0aDGhQOKJcSWGyIiIvkx3EiIs6WIiIjkx3AjIZWK4YaIiEhuDDcSsuNUcCIiItkx3Ejo2hWKORWciIhILgw3EmLLDRERkfwYbiSkrGq5MTLcEBERyYXhRkJsuSEiIpIfw42Eqq5zY+JdaomIiGRzR4SbJUuWIDg4GFqtFjExMdizZ0+dXrdq1SooFAoMGTLEugXWEVtuiIiI5Cd7uFm9ejVmzJiBOXPm4MCBA4iMjES/fv1ueiO08+fPY+bMmbj33nttVOnNqTjmhoiISHayh5uFCxfiySefxPjx49GmTRt89NFHcHR0xOeff37D1xiNRowePRrz5s1DaGioDautHVtuiIiI5CdruNHr9di/fz/i4uLMy5RKJeLi4pCYmHjD182fPx8+Pj6YMGHCTfdRXl6OgoICi4e18Do3RERE8pM13OTk5MBoNMLX19diua+vLzIyMmp8zZ9//onPPvsMn376aZ32ER8fD51OZ34EBgbWu+4bYcsNERGR/GTvlroVhYWFePzxx/Hpp5/Cy8urTq+ZNWsW8vPzzY/U1FSr1afijTOJiIhkZyfnzr28vKBSqZCZmWmxPDMzE35+ftXWP3PmDM6fP48HH3zQvMx0tQvIzs4Op06dQlhYmMVrNBoNNBqNFaqvrmoqOFtuiIiI5CNry41arUZUVBQSEhLMy0wmExISEhAbG1tt/VatWuHIkSNISkoyPx566CH07t0bSUlJVu1yqouqbikTww0REZFsZG25AYAZM2Zg7Nix6NSpE6Kjo7Fo0SIUFxdj/PjxAIAxY8YgICAA8fHx0Gq1aNeuncXr3dzcAKDacjmoOOaGiIhIdrKHm5EjRyI7OxuzZ89GRkYGOnTogA0bNpgHGaekpECpbBhDg+xUHHNDREQkN4UQd9e9AgoKCqDT6ZCfnw9XV1dJt73ucBqmrjyILqEeWDWpercaERER3Z5b+fxuGE0iDYQdZ0sRERHJjuFGQkoFx9wQERHJjeFGQhxzQ0REJD+GGwlVXeeG4YaIiEg+DDcS4pgbIiIi+THcSIjXuSEiIpIfw42E2HJDREQkP4YbCV1ruTHJXAkREdHdi+FGQua7ghvZckNERCQXhhsJccwNERGR/BhuJGR3dSq46e66owUREdEdheFGQmy5ISIikh/DjYTsOOaGiIhIdgw3EmLLDRERkfwYbiTEe0sRERHJj+FGQioFr3NDREQkN4YbCVV1S5kEYGLrDRERkSwYbiRUNRUcAIycDk5ERCQLhhsJqa6OuQE47oaIiEguDDcSqpoKDjDcEBERyYXhRkKq68INp4MTERHJg+FGQlWzpQC23BAREcmF4UZCSqUCVY03nA5OREQkD4YbiVV1TbHlhoiISB4MNxIz34KB95ciIiKSBcONxKqudWPidW6IiIhkwXAjMd48k4iISF4MNxKz45gbIiIiWTHcSIxjboiIiOTFcCMxttwQERHJi+FGYkrzmBte54aIiEgODDcSY8sNERGRvBhuJMaL+BEREcmL4UZiVde5YbghIiKSB8ONxHidGyIiInkx3EjMTsVuKSIiIjkx3EiMLTdERETyYriRmEpR1XLDqeBERERyYLiRGFtuiIiI5MVwIzGOuSEiIpIXw43EVJwKTkREJCuGG4nZsVuKiIhIVgw3EuMViomIiOTFcCMxttwQERHJi+FGYuaWGyOnghMREcmB4UZinApOREQkL4YbiXHMDRERkbwYbiRWNebGKBhuiIiI5MBwIzHzdW6MDDdERERyYLiRGGdLERERyYvhRmIcc0NERCQvhhuJseWGiIhIXgw3ErvWcsPr3BAREcmB4UZivM4NERGRvBhuJFbVLWViuCEiIpIFw43EqqaCs+WGiIhIHgw3ErNTcbYUERGRnBhuJMYxN0RERPJiuJGYHa9zQ0REJCuGG4kpFWy5ISIikhPDjcSujbnhdW6IiIjkwHAjMfOYG944k4iISBYMNxIzX+dGMNwQERHJgeFGYrzODRERkbzuiHCzZMkSBAcHQ6vVIiYmBnv27Lnhut999x06deoENzc3ODk5oUOHDvjyyy9tWG3tOFuKiIhIXrKHm9WrV2PGjBmYM2cODhw4gMjISPTr1w9ZWVk1ru/h4YFXXnkFiYmJOHz4MMaPH4/x48dj48aNNq68ZhxzQ0REJC/Zw83ChQvx5JNPYvz48WjTpg0++ugjODo64vPPP69x/V69emHo0KFo3bo1wsLC8Nxzz6F9+/b4888/bVx5zVRsuSEiIpKVrOFGr9dj//79iIuLMy9TKpWIi4tDYmLiTV8vhEBCQgJOnTqFHj161LhOeXk5CgoKLB7WdO0KxZwKTkREJAdZw01OTg6MRiN8fX0tlvv6+iIjI+OGr8vPz4ezszPUajUGDRqExYsX4/77769x3fj4eOh0OvMjMDBQ0mP4O465ISIikpfs3VK3w8XFBUlJSdi7dy/eeOMNzJgxA9u2batx3VmzZiE/P9/8SE1NtWpt5m4pTgUnIiKShZ2cO/fy8oJKpUJmZqbF8szMTPj5+d3wdUqlEs2bNwcAdOjQASdOnEB8fDx69epVbV2NRgONRiNp3bWxq5oKzgHFREREspC15UatViMqKgoJCQnmZSaTCQkJCYiNja3zdkwmE8rLy61R4i3jgGIiIiJ5ydpyAwAzZszA2LFj0alTJ0RHR2PRokUoLi7G+PHjAQBjxoxBQEAA4uPjAVSOoenUqRPCwsJQXl6OX3/9FV9++SWWLl0q52GYXbu3FMMNERGRHGQPNyNHjkR2djZmz56NjIwMdOjQARs2bDAPMk5JSYFSea2Bqbi4GJMnT8bFixfh4OCAVq1a4auvvsLIkSPlOgQL12ZLMdwQERHJQSHE3TXytaCgADqdDvn5+XB1dZV8+4dS8zB4yQ4EuDlgx0v3Sb59IiKiu9GtfH43yNlSdzJe54aIiEheDDcSuzbmRuZCiIiI7lIMNxK7dhE/phsiIiI5MNxITFV1nRsOKCYiIpIFw43EePsFIiIieTHcSIxTwYmIiOTFcCMxXqGYiIhIXgw3Ers+3NxllxAiIiK6IzDcSKxqzA3A1hsiIiI5MNxITHV9uGHLDRERkc0x3EjM7rr7YLHlhoiIyPYYbiR2fcsNZ0wRERHZHsONxCzG3BgZboiIiGyN4UZiSrbcEBERyYrhxgp4lWIiIiL5MNxYwbWrFPPmmURERLbGcGMFarvKt1VvYLghIiKyNYYbK3CwVwEAyioYboiIiGyN4cYKHNSV4aa0wihzJURERHcfhhsruNZyw3BDRERkaww3VqC9Gm5K9Qw3REREtsZwYwVVLTfsliIiIrK92wo3qampuHjxovn7PXv2YPr06fjkk08kK6wh45gbIiIi+dxWuHnsscewdetWAEBGRgbuv/9+7NmzB6+88grmz58vaYENEcfcEBERyee2ws3Ro0cRHR0NAPj222/Rrl077Ny5E19//TWWL18uZX0NEsfcEBERyee2wk1FRQU0Gg0AYMuWLXjooYcAAK1atUJ6erp01TVQDurKt5XdUkRERLZ3W+Gmbdu2+Oijj/DHH39g8+bN6N+/PwAgLS0Nnp6ekhbYEHFAMRERkXxuK9z8+9//xscff4xevXph1KhRiIyMBAD89NNP5u6qu5l5zA27pYiIiGzO7nZe1KtXL+Tk5KCgoADu7u7m5ZMmTYKjo6NkxTVUWs6WIiIiks1ttdyUlpaivLzcHGwuXLiARYsW4dSpU/Dx8ZG0wIboWrcU7y1FRERka7cVbgYPHowvvvgCAJCXl4eYmBi8++67GDJkCJYuXSppgQ2RA2dLERERyea2ws2BAwdw7733AgDWrl0LX19fXLhwAV988QU++OADSQtsiKou4sfr3BAREdnebYWbkpISuLi4AAA2bdqEYcOGQalUokuXLrhw4YKkBTZEWs6WIiIiks1thZvmzZvjhx9+QGpqKjZu3Ii+ffsCALKysuDq6ippgQ0Ru6WIiIjkc1vhZvbs2Zg5cyaCg4MRHR2N2NhYAJWtOB07dpS0wIaI3VJERETyua2p4CNGjED37t2Rnp5uvsYNAPTp0wdDhw6VrLiGihfxIyIiks9thRsA8PPzg5+fn/nu4E2bNuUF/K7S2vP2C0RERHK5rW4pk8mE+fPnQ6fTISgoCEFBQXBzc8OCBQtgMvHaLrxxJhERkXxuq+XmlVdewWeffYY333wT3bp1AwD8+eefmDt3LsrKyvDGG29IWmRDU9UtVW4wwWQSUCoVMldERER097itcLNixQr897//Nd8NHADat2+PgIAATJ48meHm6oBiACgzGOGovu3ePyIiIrpFt9UtlZubi1atWlVb3qpVK+Tm5ta7qIZOa3ct3LBrioiIyLZuK9xERkbiww8/rLb8ww8/RPv27etdVEOnVCqgseOgYiIiIjncVn/JW2+9hUGDBmHLli3ma9wkJiYiNTUVv/76q6QFNlQOahXKDSZe64aIiMjGbqvlpmfPnvjrr78wdOhQ5OXlIS8vD8OGDcOxY8fw5ZdfSl1jg3TtKsWcPUZERGRLtz3StUmTJtUGDh86dAifffYZPvnkk3oX1tDxQn5ERETyuK2WG7o53jyTiIhIHgw3VlI1HZyzpYiIiGyL4cZKqrqlOKCYiIjItm5pzM2wYcNqfT4vL68+tTQq7JYiIiKSxy2FG51Od9Pnx4wZU6+CGgt2SxEREcnjlsLNsmXLrFVHo+PAO4MTERHJgmNurIRjboiIiOTBcGMlWnZLERERyYLhxkp4ET8iIiJ5MNxYCcMNERGRPBhurKRqthTH3BAREdkWw42VmK9zwzE3RERENsVwYyXsliIiIpIHw42VXAs3JpkrISIiursw3FiJecwNu6WIiIhsiuHGSnhvKSIiInkw3FgJx9wQERHJg+HGStgtRUREJA+GGythyw0REZE87ohws2TJEgQHB0Or1SImJgZ79uy54bqffvop7r33Xri7u8Pd3R1xcXG1ri+XqnBjMAlUGDljioiIyFZkDzerV6/GjBkzMGfOHBw4cACRkZHo168fsrKyalx/27ZtGDVqFLZu3YrExEQEBgaib9++uHTpko0rr51Wfe2tZesNERGR7SiEEELOAmJiYtC5c2d8+OGHAACTyYTAwEBMmzYNL7300k1fbzQa4e7ujg8//BBjxoy56foFBQXQ6XTIz8+Hq6trveu/ESEEwl7+FSYB7Hm5D3xctVbbFxERUWN3K5/fsrbc6PV67N+/H3FxceZlSqUScXFxSExMrNM2SkpKUFFRAQ8PjxqfLy8vR0FBgcXDFhQKBcfdEBERyUDWcJOTkwOj0QhfX1+L5b6+vsjIyKjTNl588UU0adLEIiBdLz4+HjqdzvwIDAysd911VTVjiuGGiIjIdmQfc1Mfb775JlatWoXvv/8eWm3N3T6zZs1Cfn6++ZGammqz+njzTCIiItuzk3PnXl5eUKlUyMzMtFiemZkJPz+/Wl/7zjvv4M0338SWLVvQvn37G66n0Wig0WgkqfdWsVuKiIjI9mRtuVGr1YiKikJCQoJ5mclkQkJCAmJjY2/4urfeegsLFizAhg0b0KlTJ1uUelvMF/JjuCEiIrIZWVtuAGDGjBkYO3YsOnXqhOjoaCxatAjFxcUYP348AGDMmDEICAhAfHw8AODf//43Zs+ejZUrVyI4ONg8NsfZ2RnOzs6yHUdNrnVL8To3REREtiJ7uBk5ciSys7Mxe/ZsZGRkoEOHDtiwYYN5kHFKSgqUymsNTEuXLoVer8eIESMstjNnzhzMnTvXlqXfFLuliIiIbE/2cAMAU6dOxdSpU2t8btu2bRbfnz9/3voFScTxardUid4gcyVERER3jwY9W+pO56SpzI5F5Qw3REREtsJwY0XOV8NNMcMNERGRzTDcWFFVuCkqY7ghIiKyFYYbK7rWLcUBxURERLbCcGNFzprKAcXsliIiIrIdhhsrqmq5KeZsKSIiIpthuLGiqjE3hRxzQ0REZDMMN1bE2VJERES2x3BjRU4MN0RERDbHcGNFvIgfERGR7THcWJGLtmpAsRFCCJmrISIiujsw3FhRVcuN0SRQVsE7gxMREdkCw40VOV69KzjArikiIiJbYbixIqVSASc1L+RHRERkSww3VsZBxURERLbFcGNlzlqGGyIiIltiuLEyXsiPiIjIthhurMxJzZYbIiIiW2K4sbJrVyk2ylwJERHR3YHhxspczGNuKmSuhIiI6O7AcGNlTprKqeBFbLkhIiKyCYYbK+PNM4mIiGyL4cbKnNUMN0RERLbEcGNlVde5KWS4ISIisgmGGytjtxQREZFtMdxYGS/iR0REZFsMN1Z27d5SnC1FRERkCww3Vuas4XVuiIiIbInhxsqceYViIiIim2K4sbJrF/HjmBsiIiJbYLixsqqWG73BhAqjSeZqiIiIGj+GGyurGlAMcMYUERGRLTDcWJm9SgmNXeXbXFjGcENERGRtDDc2YB5UrGe4ISIisjaGGxvgVYqJiIhsh+HGBnghPyIiItthuLEBl6pwwzE3REREVsdwYwNV17phtxQREZH1MdzYwLVuKYYbIiIia2O4sQHeGZyIiMh2GG5swJktN0RERDbDcGMD7JYiIiKyHYYbG2DLDRERke0w3NiAu5MaAJBbrJe5EiIiosaP4cYG/Fy1AICM/DKZKyEiImr8GG5swE+nAcBwQ0REZAsMNzbgp3MAABSWGzgdnIiIyMoYbmzAWWNnHlScUcDWGyIiImtiuLERX9fKrqlMdk0RERFZFcONjfjprg4qZssNERGRVTHc2Iifa+W4m3S23BAREVkVw42NVM2YymTLDRERkVUx3NgIr3VDRERkGww3NuJ7Ndyw5YaIiMi6GG5sxF/HMTdERES2wHBjI75Xx9zkFJXDYDTJXA0REVHjxXBjI15OGtgpFTAJILuoXO5yiIiIGi2GGxtRKhXwceE9poiIiKyN4caGqi7kx0HFRERE1sNwY0NV4YaDiomIiKyH4caGqqaD8xYMRERE1sNwY0NVF/LjzTOJiIish+HGhnjzTCIiIutjuLEhc8tNAaeCExERWYvs4WbJkiUIDg6GVqtFTEwM9uzZc8N1jx07huHDhyM4OBgKhQKLFi2yXaESCPRwBACk5pagRG+QuRoiIqLGSdZws3r1asyYMQNz5szBgQMHEBkZiX79+iErK6vG9UtKShAaGoo333wTfn5+Nq62/vx1WjTRaWEwCRxMyZO7HCIiokZJ1nCzcOFCPPnkkxg/fjzatGmDjz76CI6Ojvj8889rXL9z5854++238eijj0Kj0di42vpTKBSICfUEAOw+e1nmaoiIiBon2cKNXq/H/v37ERcXd60YpRJxcXFITEyUbD/l5eUoKCiweMgpOsQDALD7XK6sdRARETVWsoWbnJwcGI1G+Pr6Wiz39fVFRkaGZPuJj4+HTqczPwIDAyXb9u2oCjcHU/NQbjDKWgsREVFjJPuAYmubNWsW8vPzzY/U1FRZ6wn1coKXswZ6gwmHUvNlrYWIiKgxki3ceHl5QaVSITMz02J5ZmampIOFNRoNXF1dLR5yUigUiLnaerPnHMfdEBERSU22cKNWqxEVFYWEhATzMpPJhISEBMTGxspVlk1w3A0REZH12Mm58xkzZmDs2LHo1KkToqOjsWjRIhQXF2P8+PEAgDFjxiAgIADx8fEAKgchHz9+3Pz1pUuXkJSUBGdnZzRv3ly247hVMaGV4Wb/hSuoMJpgr2r0vYNEREQ2I2u4GTlyJLKzszF79mxkZGSgQ4cO2LBhg3mQcUpKCpTKax/8aWlp6Nixo/n7d955B++88w569uyJbdu22br829bCxwXujva4UlKBXw6nY0jHALlLIiIiajQUQgghdxG2VFBQAJ1Oh/z8fFnH33z4WzLe2fQXPJ3USHi+J9wc1bLVQkREdKe7lc9v9ofIZFKPMIT7OONysR7xv56UuxwiIqJGg+FGJmo7Jf41LAIAsHpfKvZf4OBiIiIiKTDcyKhzsAceimwCANhyoub7aREREdGtYbiRWcdmbgCAs9lF8hZCRETUSDDcyCzU2xkAcDa7WOZKiIiIGgeGG5mFejkBAM5fLobBaJK5GiIiooaP4UZmAW4O0NgpUWEUuHilVO5yiIiIGjyGG5kplQqEXG29OZvDcTdERET1xXBzBwi7Ou7mTBbH3RAREdUXw80dINSbLTdERERSYbi5A5hbbjhjioiIqN4Ybu4A5pYbhhsiIqJ6Y7i5A1QNKM4pKkd+aYXM1RARETVsDDd3ABetPXxdNQB4pWIiIqL6Yri5Q4R68UrFREREUmC4uUNUjbs5w5YbIiKiemG4uUNUzZg6cilf5kqIiIgaNoabO0T3cC8oFcAfyTn4Izlb7nKIiIgaLIabO0QLXxeMiQ0GALz2w1GUVRjlLYiIiKiBYri5gzzftwV8XDQ4f7kES7edkbscIiKiBonhRiomI5B/Ecg6edubcNHaY86DbQEAH20/g/wSXvOGiIjoVjHcSOXsNuC9tsDaJ+q1mYERfmjt74pygwnfH7woTW1ERER3EYYbqegCK//NT63XZhQKBR7tXLmtVXtTIYSob2VERER3FYYbqegCKv8tLwDK6jede0iHAGjslDiZUYhDFzk1nIiI6FYw3EhF7QQ4eFR+nV+/7iSdoz0GRvgDAFbtSalvZURERHcVhhspuVV1TdV/rExV19RPh9JQVG6o9/aIiIjuFgw3Uqoad5NX/9aW6BAPhHo5oURvxLpDafXeHhER0d2C4UZKuqaV/0rQcqNQKDDyauvNN3uvDVLOKixDuYEX+CMiIroRhhspSRhuAGB4VFPYKRU4lJqHE+kF2HYqC13jf8PUlQcl2T4REVFjxHAjJYnDjZezBve38QVQeVG/F9YehsEksPl4Jv7KLJRkH0RERI0Nw42UdNINKK7yaHQzAMCPSWnILiw3L1+247xk+yAiImpMGG6kVBVuCtMAozS3Tuje3AsBbg4AADulAq8Oag0A+P7gReSV6CXZBxERUWPCcCMlJ29ApQaECShMl2STKqUCE+8NAQDM7NcSE7qHoLW/K8oqTFh13UBjg9GEgylXYDCaJNkvERFRQ8VwIyWlEnC9eqViCbumxnUNxv5X4/B0zzAoFAqM7xYMAPj8z3M4nlaA/JIKjPl8D4b+ZycmrNiHCgYcIiK6izHcSE3iQcVA5bRwT2eN+fuHIpsg0MMBWYXlGLzkTwx4/3fsPHMZALD9r2y8+v1R3pOKiIjuWgw3UpPoBpq10dqr8P3kbujbxhcVRoG0/DIEuDng1UGtoVQAq/el4sPfTltt/0RERHcyO7kLaHSqWm7yrBdugMpp4h8/HoWfD6fjwIUrmNK7ObxdNNDYq/DaD0fx3pa/0LW5J6KCPKxaBxER0Z2GLTdSk/D+UjejUCjwUGQTzH2oLbxdKrutHu8ShGEdA2ASwD9WH0JxuQFCCBhNNXdTmUwCZRW84jERETUebLmRmhXG3NyquYPbYtfZy0jJLcHITxKRU6hHboke47sGY3pcCzioVRCi8mKAC345joJSAz4f14mtPERE1CgoxF028rSgoAA6nQ75+flwdXWVfgc5ycCHnQC1MzDrIqBQSL+POth5OgeP/Xd3teWBHg5o5eeK9PxSHL1UYF7urLHDlxOi0bGZuy3LJCIiqpNb+fxmy43UqqaC64uA0iuAozytIV2be+H9RzvgVEYhuoZ5oVhvwNyfjiE1txSpuaUAALVKiYn3huBgSh4Sz17GmM/2YMWEaNxTh4CTW6xHUZkBjhoVPBzVUCqvhbhSvREOapXVjo2IiKg2DDdSUzsCHqFA7lng+A9ApydkK2VwhwCL77uGeeKXw+kwCgEXrT3uaeaGpu6OKNEbMO7zvdhzPhejP92Njx6PQs8W3ubXJaXm4cjFPDwY2QTOGjt88NtpfPhbMqqG8bTwdcYXT8TAT6fFBwnJ+CAhGdPuC8dzceF1rrXCaMKZ7CIAgIejGj6u2hrXSUrNw+6zl3E8vQBKhQIO9ioMau+PXi19buMdAsoqjNDaM4gRETUm7JayhsT/ABtnAZ7NgSl7Ky/ud4crLjfg6a/244/kHNirFJg1oDX+r0sQfj2SjhfWHkKFUcDBXoVmHo44dfWmnQ72KpReHYwc7uOMofcE4K0Np8zb/HJCNGJCPPHJ72dw9FIB3J3UCHDTYmCEP0K9nc3rZeSXYdSnu3AupxgAoFQAcx9qizGxwQAAvcGEtfsvYsnW07iUV1pj/WNjgzBrYOs6B5ULl4vx9sZT+OVIOkbHNMP8h9pZtD7V5kDKFRy9lI/mPs5o66+DztEeAJBdWI5/rj0EtZ0Sbw2PNC+vC6NJQFXH/cuFQZCI5HQrn98MN9ZQXggsbAuU5wOjVgMt+1tnPxLTG0x4fs0h/HwoDQDg56pFRkEZgMqp5zlFlTfudFKr8K9hERjcIQCpuSV4+KNE83oAEObthDPZxfBy1iDATYtDF/Or7SsqyB0PRzVFbJgnnli+F2eyi+Fgr4KDWoXcYj2UCmDFE9FwsFdh5ppDOH+5BADg7miPrmFe6NjMDSqlAn9lFuGbPSkAKgPW7Afb4N7wylanCqMJdkoFhAA2n8jEZ3+cw+nsIqhVSlwuLkeF8dqP/mMxzfD64NoDzpViPf716wms2X9tsLhCATzQvgmGdQzA7J+Omrv8Wvq64OPHo3AsrQDbTmXhTHYRUnJLoFAo4Kq1Q0s/FzzdMwxBHk54e9NJfLv3IobdE4C5D7W1CBDlBiMSTmRh+6lsHEvPR7cwL0zrEw5njR0u5ZWiVG9Ecx9nizr3nMvFG7+egKvWDv8ZfQ9ctDcOWQajCV/vTsGBlCuY2rs5wn1dqq2TXViOmWsOYeeZHMx+sC0e7xJ0w+1VEUJAUcN4s/M5xdhzLheh3k6IaKpDYZkBRy7m4/DFfBy5lIeCUgMGRPhhaMcAuDmqb7ofIrp7MNzUwibhBgA2vQbs/AAIvhcYt856+5GYySTw9Z4UvL8l2RxmJnYPwayBrbHr7GX8kZyDRzsHItjLyfya5MxCPPxxIvJKKjAquhlmP9AGg5f8ib8yK7uZdA72eKpnKPQGEw5fzMf2v7KrTU3312nx7VOxaOrugBfWHsba/RfhqFahrMIIkwC8XTSY3CsMo6KbVWs92HoqCzO/PYTLxZU3Em3t74rc4nJkFpRDrVJCa69EQZmh2rH2aOGN2FBPvLXxJIQAokM80CnIHYEejlApFBAQKCo34kqxHgdSruBAyhWUVVTe2iI21BOX8kqRkltisc0gT0eU6o3Iuu4O7rVx1tihqPxabREBOnz4WEcEeToh5XIJnvpqP06kF1i8xsdFA3/dtdC4YHBbPB4bjNxiPeL/Fr5iQz2xbHxnHLhwBVtPZSGiqRviWvvAaBLYdTYX7246hZMZlS1xajslpvZujisleiScyILWXokOgW747WS2+WcBAOY82Abju4Xc8Jh+TLqEBeuO46HIALz2QGsoFArsOnsZ729JRuLZy+b1VErFDS9RoLZTondLbwyM8Ee35l7wuu4K3XVVVG5AUkoeWvq5mC+V8HdHL+Xj8z/PoWdL72rduLcis6AMr/9yAkEejpgeFw471a211uYUlWP90Qy4au1wTzN3NHV3MIfDEr0BeSUV8NdpawyMVa4U6+HmaF/rOkQNGcNNLWwWbvIvAe+3B0wG4PEfgLDe1tuXFRSXG7Bqbyq8nNV1+qWfmluCI5fy0a+t39UWlUKM+WwPQr2d8M7DkWhy9c7mAJBVUIbvDl7Cmn2p5haeb5/qYu6qKjcYMeqTXTiQkgcAGH5PU8x9qE2tLRD5JRV4PyEZXySeh6GGD0xXrR0ejw3CgHb+AAAHtQphV/f3v/0XMXPtIdTlf0JLXxe8MbQdOgVXDhQ/nlaA9xP+wsZjmejYzA3/HdMJJXojxn6+B2dzitHU3QGDIvwR0VSHYE8nKBUKXCnR43/7L+KHpEswicqWrse7BOH9hGRcKamAQgFEB3vgZEYh8ksr4OGkxtCOAWju44yPt58xt2Jdb/g9TfHbyUxcKam8G/2QDk2w5UQWisoN8HJWI6fo2h3kNXZKVBhN5jFTbo72aOHrgj3ncms97k7B7vh6d4p5f2O7BsHBXoXfk3NwpViPdgGuOJFeiPcTks2ve3lgKwR7OmHKygOoMAooFECHQDek5pYgp0gPhQII83ZG+wAdIprqAADf7rtYLdB5u2gQ2VSHHi280cbfFcfTC3A6qwhRQe4Y0M4f5QYjfkxKw1+ZhVCrlMgpKsfGY5korTDCSa3CP+5vgXFdg2GnUuJyUTmOpRXg50NpWHvgovm8LxjSDg9HNcWKnedxMqMQ/dr64f42vjAJgb8yCytbmC7l43JROexUSrho7BAb5gk3RzVmrjmE7KuB9t5wL3z42D3QOdgjv6QCX+2+gJ+S0uDprEZUkDv6tfVDu4DKY72UV4r3Nv+Fn5LSoL/unnBaeyVcr/68VwXlzsHumD+4HVr7X/u9pTeYsPd8Lj787TQSz17GwAg/LHykQ43dh6m5Jfgi8Ty09iq09HNBx2buCHBzQF6JHq//cgKbjmUgMtANI6KaIirIHT4uWqjtbi2klVUY8VdmIZr7OMNRfWtDOssqjNh7PhfH0grgpFbBzVGNzsEe8NNVH3/3d3qDCeUGY62/I6ocuZiPI5fy0aOFF5q6OwKo/KNOocBNg6EQAj8mpSEpNQ8T7w0xv14O+aUVSM0tQWt/V6iUCphMAhevlMLHVdNou48Zbmphs3ADAD9MBpK+Buy0wIjPgVaDrLu/O8yNuiauf/5kRiG8XTTV/jLPKSrHh7+dRtcwT/Rt61fnfV64XIxDF/MR6O6AZh6OKK0woqDUgCBPRzhpbvzLNjmzELvO5eJ4WgGyC8vMH/zOGjs4a+3Q2t8VXUI80NzHucZjyikqt5g1VqI34NKV0huuDwDncopxPK0A97fxhdpOiYtXSvDS/47gz9M55nU6BLph6f/dA39dZTis+hA3mgTiWvviy10X8MF1YaKVX2X4igryQOKZyxi7bA/0BhPUdkr0beOLQxfzzF1nzTwc0ae1D569LxxujvZYtTcVq/amItTLCQMj/CGEwIGUPOgc7DG+WzA0dkos3PwXFtfh1h6xoZ5IPHsZCgWgUihgMAkMaOeHVx9ogwA3BwghcCmvFG6Oajj/7bwIIXA8vQAbjmZg47EMJGcV1Ro8vZw1KNEbUKKvfjFKV61dja1214sI0OHIpXzztq5vpfJyVqOgzAC94eY3ow31ckJ6fhlKK4xw1drBRWuPy8Xl5ta+KkoFMO2+cHQIdMM/vk1C3tVAGhGgg1IBHEsrqBbQFQpAiMrWrkB3BxSVG1BYZkB5DXXFhHigfzs/rNl3EcV6A4Z1bAp/nRYL1h1HYbnle9Ha3xU5ReXmYPb3fTbROaClnwt8XTXILixHQZkB9zb3wvCopvBwUiM1twQpuSW4cLkESal5SDiRiWK9ES5aOwy/pym6hHrCSaOCr6sWzb2doVQqcDytAJuPZ8JBrYS/zgEpuSXYcToH+y5cqfY+KxXAveHeGNoxAD1aeEMB4KtdF/BHcg4imurQp7UPdp6+jC8Sz6OgzAA/Vy1a+bugc7AHIpu6IS2/FCfTK1smvVzU2H02F9v/yjZvu0cLbxSVGXD4Uj48ndQYfk9TPBDpj2BPp2oBoVRvxGs/HsXaqy2jGjslnukVhiEdAhDk6QiFQgEhBIr1RuSXViCvRI/80goUlRngpLGDzsEevq5aeDmrkZ5fhu8PXsLprCL0aumNfm39oLVXmX8n/nYyC2l5pfBx0cJFa4eLV0qRUVCKrmFeeLRzIA5fysczX+1HZkE5PJzU6BDohsMX85BTpIevqwavDGqDqCB3/HwoDZkFZRh+T1NzoC7VG3H4Yh4OpubBzcEew6Oawv4mLY0Xr5QgOasIMSEeNwytBqMJuSV6OKrt4KRWWaUFkeGmFjYNN/oSYO144K8NgEIJ3Pcq0PVZQFX3gaZ097l4pQQ/H0qH0WTCkz1CobGr/a+w5TvOYUXiBTzaORBPdA+x+EW173wudp/LxYiopvB11UIIgeSsIvMv2tux73wuvtx1AeuPZFS2MoV4oInOAYcv5SO/RI+p94VjVHQgXv3hqLmlZ3CHJnj34chb7q4BKlsRT2UWYtfZy9h2KhvncorR2t8VwZ6O2HA0w9yy0dzHGXGtfSEgoFIo0Ke1LzoEumHNvlT8e8NJc6sWAIR4OSEiQIexXYNxTzM3vLn+JD7+/SwAoIlOi7g2vvj5UJr5NS5aO7RvqkO7AB2aujvCaDQhPb8M205l41RmIQZF+OOtEe1xLqcYT36xD+n518agtfJzwRPdQ2AwCvx2MgtbTmRaHF9EgA7zBrc1X4KhrMKI7MJy5JdWwGgSCPJ0RIneiNd/OY5fj2RUe38c7FUY2TkQUUHumPXdEYtuzr/r2MwNLX1dcCK9AEcu5ZtDfJi3E14a0BpHL+XjlyPpSLlcYtGS9HdVYasmWntltUAHAB5Oang7a8wTEmrir9PiniB3GK6+v4evG6+nUAD2KmWdgmZtVEoFWvm54FhaQa3reTmr0cTNAX6uWuSVVuBMVhEuXx0P2MqvsvWwiqvWDvYqJfJLK2psOb6e1l6JcoPJ4v1z0drBRWOHwjJDtQD6d8GejkjLK4PeaKr1PPxdt+aeKCg14ES6ZXhu4++KF/q3xIn0Auw5lwtXrT0CPRzgYK+C3mDC/pQr2HnmMoSofE+e7hmGrmFecNHa4VhaATYdy8CBlCu4eKXUvF2VUoFOQe5Y/VRs3YqrI4abWtg03ACA0QCsmw4c/LLye+/WQL/XgdD7GsQsKqIbKdUboVDghk3gFUYT3t54Co5qFabdF26V2WAVRhN2nM6Bs8YOUUHuN/xrscJoQn5pZVBxVKuq/fUphMCqvako1RvxWEzluK6yCiMOpFxBE52D+S/zmhSXGyxaBcsqjEjOLIJRCGjtlWjp62Lx2h8OXsKrPxxFUbkBo6KbYc6DbercjfBXZiEKSivgrLWDs8YOLhp7OGlU5tB49FI+pqw8ACe1HR6NDoTOwR7Ld57H0Uv5eKZXczx7X3PzurnFemw7lQWDSeChyCYWNQghcLlYj7PZxTiVUYCcIj18XDVQKhT4MekSdp2t7MJ00dihmacjmnk4IszbGX1a+6B9UzfsOJ2DNfsv4tKVEpTojbhwucQ8s9JepUDvlj7Q2quQllcKDyc1uod7oVtzL4R6OVm8V+dyivG//Rex5USmeWxYuwBXDL+nKQ6m5OH35GyEeDnhqR6h6BLqiTPZRThyMR+7z1V2bzVx06KNvw72dgpkF5bDzUGNsV2DEOTphNNZhdh4LBO+rlp0CHTDyYwCfLvvIvafz0VxDa2AQOWH+wePdkRsmCfWHU7Hsh3ncDStoFrgUquUcHWwh5ujPZw0dijVG3ClpAI5ReXmMBIT4oH2TXX45XA60q4Lwxo7Jbo39zK3quWXViDAzQFOGjt8uesCcq+OLezf1g//Ht4ex9MLcCwtH22b6NAuwBXLdpzHkq2nUW4wISbEA17OGqw/mo7rM5ePiwYdAt2w53yuueXwZjyd1OZxjXXRJdQDqyYx3NiMzcMNUBmtD30DbHoVKLk6oNIjFGg/EvBtC3i1ANxDADvODiG6G2QVlCEtvwwdAt1ssj+D0XRbrWY3kl1YDpVSAfc6DmDWG0w4cikfF6+UoHtzL3jexgDxjPwy5JdWoIXvjbt6pSCEQEGpARfzSnDpSikyCsqgc7BHoIcjWvu5VrtAqd5QeY0uhaJy8oSbgxpae2WNNeoNJqTllcJBrTK3nBpNAkevdos6aezQ1N3hhmE3v7QCX+w8DzdHe4yOCbrh7M7CsgqUG0zm7v6z2UXYdDwTAW4OuCfIHU2uDk7PLizH3J+PYcfpHEQ2dUOvlt6oMJqQkluCCoOA2k4JP50WD0U2gZ9Oi+8OXMQXiReQWVCOgtIK+Om0uL+NL3q19EaYtzP8XLUoN5hQUFbZ6nj9WEspMNzUQpZwU6UkF9j+b+Dg14D+b02zChXgHlx5402XJoBWVxl2VGpApbnu66sPOw2gVFW+TqGs/lDWsEyh/Nv6ihqet/ZMCxvM5Gjox2CT2S6N4RisjD9HddmJlTffCH6O7lYqDeDiK+kmGW5qIWu4qVJeBBz7Djj/J5DzV+X9qPRF8tRCREQktabRwMTNkm6S95a602mcgXvGVD6Aym6rwvTKkFOQBhSmVV4I0FgBGMoBo/7aw1BeudxYDpiMgDBde/z9eyEA8fdlVeuJvy2/up4t2DRP23BfNjsuWx5To9tRIz1PjfD/1N31d3fjY3frXY+S7l7WvVMlhQJwbVL5ICIionrhdB0iIiJqVBhuiIiIqFFhuCEiIqJGheGGiIiIGhWGGyIiImpUGG6IiIioUWG4ISIiokaF4YaIiIgaFYYbIiIialQYboiIiKhRuSPCzZIlSxAcHAytVouYmBjs2bOn1vXXrFmDVq1aQavVIiIiAr/++quNKiUiIqI7nezhZvXq1ZgxYwbmzJmDAwcOIDIyEv369UNWVlaN6+/cuROjRo3ChAkTcPDgQQwZMgRDhgzB0aNHbVw5ERER3YkUQsh769WYmBh07twZH374IQDAZDIhMDAQ06ZNw0svvVRt/ZEjR6K4uBjr1q0zL+vSpQs6dOiAjz766Kb7u5VbphMREdGd4VY+v2VtudHr9di/fz/i4uLMy5RKJeLi4pCYmFjjaxITEy3WB4B+/frdcP3y8nIUFBRYPIiIiKjxspNz5zk5OTAajfD19bVY7uvri5MnT9b4moyMjBrXz8jIqHH9+Ph4zJs3r9pyhhwiIqKGo+pzuy4dTrKGG1uYNWsWZsyYYf7+0qVLaNOmDQIDA2WsioiIiG5HYWEhdDpdrevIGm68vLygUqmQmZlpsTwzMxN+fn41vsbPz++W1tdoNNBoNObvnZ2dkZqaChcXFygUinoegaWCggIEBgYiNTW1UY7naezHB/AYG4PGfnwAj7ExaOzHB0h/jEIIFBYWokmTJjddV9Zwo1arERUVhYSEBAwZMgRA5YDihIQETJ06tcbXxMbGIiEhAdOnTzcv27x5M2JjY+u0T6VSiaZNm9a39Fq5uro22h9WoPEfH8BjbAwa+/EBPMbGoLEfHyDtMd6sxaaK7N1SM2bMwNixY9GpUydER0dj0aJFKC4uxvjx4wEAY8aMQUBAAOLj4wEAzz33HHr27Il3330XgwYNwqpVq7Bv3z588sknch4GERER3SFkDzcjR45EdnY2Zs+ejYyMDHTo0AEbNmwwDxpOSUmBUnltUlfXrl2xcuVKvPrqq3j55ZcRHh6OH374Ae3atZPrEIiIiOgOInu4AYCpU6fesBtq27Zt1ZY9/PDDePjhh61c1a3TaDSYM2eOxRifxqSxHx/AY2wMGvvxATzGxqCxHx8g7zHKfhE/IiIiIinJfvsFIiIiIikx3BAREVGjwnBDREREjQrDDRERETUqDDcSWbJkCYKDg6HVahETE4M9e/bIXdJti4+PR+fOneHi4gIfHx8MGTIEp06dslinV69eUCgUFo+nn35apopvzdy5c6vV3qpVK/PzZWVlmDJlCjw9PeHs7Izhw4dXuyr2nS44OLjaMSoUCkyZMgVAwzx/v//+Ox588EE0adIECoUCP/zwg8XzQgjMnj0b/v7+cHBwQFxcHJKTky3Wyc3NxejRo+Hq6go3NzdMmDABRUVFNjyKG6vt+CoqKvDiiy8iIiICTk5OaNKkCcaMGYO0tDSLbdR03t98800bH8mN3ewcjhs3rlr9/fv3t1jnTj6HwM2Psab/lwqFAm+//bZ5nTv5PNbl86Euv0NTUlIwaNAgODo6wsfHBy+88AIMBoNkdTLcSGD16tWYMWMG5syZgwMHDiAyMhL9+vVDVlaW3KXdlu3bt2PKlCnYtWsXNm/ejIqKCvTt2xfFxcUW6z355JNIT083P9566y2ZKr51bdu2taj9zz//ND/3j3/8Az///DPWrFmD7du3Iy0tDcOGDZOx2lu3d+9ei+PbvHkzAFhcQqGhnb/i4mJERkZiyZIlNT7/1ltv4YMPPsBHH32E3bt3w8nJCf369UNZWZl5ndGjR+PYsWPYvHkz1q1bh99//x2TJk2y1SHUqrbjKykpwYEDB/Daa6/hwIED+O6773Dq1Ck89NBD1dadP3++xXmdNm2aLcqvk5udQwDo37+/Rf3ffPONxfN38jkEbn6M1x9beno6Pv/8cygUCgwfPtxivTv1PNbl8+Fmv0ONRiMGDRoEvV6PnTt3YsWKFVi+fDlmz54tXaGC6i06OlpMmTLF/L3RaBRNmjQR8fHxMlYlnaysLAFAbN++3bysZ8+e4rnnnpOvqHqYM2eOiIyMrPG5vLw8YW9vL9asWWNeduLECQFAJCYm2qhC6T333HMiLCxMmEwmIUTDPn9CCAFAfP/99+bvTSaT8PPzE2+//bZ5WV5entBoNOKbb74RQghx/PhxAUDs3bvXvM769euFQqEQly5dslntdfH346vJnj17BABx4cIF87KgoCDx3nvvWbc4idR0jGPHjhWDBw++4Wsa0jkUom7ncfDgweK+++6zWNaQzuPfPx/q8jv0119/FUqlUmRkZJjXWbp0qXB1dRXl5eWS1MWWm3rS6/XYv38/4uLizMuUSiXi4uKQmJgoY2XSyc/PBwB4eHhYLP/666/h5eWFdu3aYdasWSgpKZGjvNuSnJyMJk2aIDQ0FKNHj0ZKSgoAYP/+/aioqLA4n61atUKzZs0a7PnU6/X46quv8MQTT1jcLLYhn7+/O3fuHDIyMizOm06nQ0xMjPm8JSYmws3NDZ06dTKvExcXB6VSid27d9u85vrKz8+HQqGAm5ubxfI333wTnp6e6NixI95++21Jm/ptYdu2bfDx8UHLli3xzDPP4PLly+bnGts5zMzMxC+//IIJEyZUe66hnMe/fz7U5XdoYmIiIiIizHciAIB+/fqhoKAAx44dk6SuO+IKxQ1ZTk4OjEajxUkCAF9fX5w8eVKmqqRjMpkwffp0dOvWzeIWF4899hiCgoLQpEkTHD58GC+++CJOnTqF7777TsZq6yYmJgbLly9Hy5YtkZ6ejnnz5uHee+/F0aNHkZGRAbVaXe0Dw9fXFxkZGfIUXE8//PAD8vLyMG7cOPOyhnz+alJ1bmr6f1j1XEZGBnx8fCyet7Ozg4eHR4M7t2VlZXjxxRcxatQoixsSPvvss7jnnnvg4eGBnTt3YtasWUhPT8fChQtlrLbu+vfvj2HDhiEkJARnzpzByy+/jAEDBiAxMREqlapRnUMAWLFiBVxcXKp1ezeU81jT50NdfodmZGTU+H+16jkpMNxQraZMmYKjR49ajEkBYNHHHRERAX9/f/Tp0wdnzpxBWFiYrcu8JQMGDDB/3b59e8TExCAoKAjffvstHBwcZKzMOj777DMMGDAATZo0MS9ryOfvbldRUYFHHnkEQggsXbrU4rkZM2aYv27fvj3UajWeeuopxMfHN4jL/D/66KPmryMiItC+fXuEhYVh27Zt6NOnj4yVWcfnn3+O0aNHQ6vVWixvKOfxRp8PdwJ2S9WTl5cXVCpVtZHgmZmZ8PPzk6kqaUydOhXr1q3D1q1b0bRp01rXjYmJAQCcPn3aFqVJys3NDS1atMDp06fh5+cHvV6PvLw8i3Ua6vm8cOECtmzZgokTJ9a6XkM+fwDM56a2/4d+fn7VBvkbDAbk5uY2mHNbFWwuXLiAzZs3W7Ta1CQmJgYGgwHnz5+3TYESCw0NhZeXl/nnsjGcwyp//PEHTp06ddP/m8CdeR5v9PlQl9+hfn5+Nf5frXpOCgw39aRWqxEVFYWEhATzMpPJhISEBMTGxspY2e0TQmDq1Kn4/vvv8dtvvyEkJOSmr0lKSgIA+Pv7W7k66RUVFeHMmTPw9/dHVFQU7O3tLc7nqVOnkJKS0iDP57Jly+Dj44NBgwbVul5DPn8AEBISAj8/P4vzVlBQgN27d5vPW2xsLPLy8rB//37zOr/99htMJpM53N3JqoJNcnIytmzZAk9Pz5u+JikpCUqlslpXTkNx8eJFXL582fxz2dDP4fU+++wzREVFITIy8qbr3knn8WafD3X5HRobG4sjR45YBNWqsN6mTRvJCqV6WrVqldBoNGL58uXi+PHjYtKkScLNzc1iJHhD8swzzwidTie2bdsm0tPTzY+SkhIhhBCnT58W8+fPF/v27RPnzp0TP/74owgNDRU9evSQufK6ef7558W2bdvEuXPnxI4dO0RcXJzw8vISWVlZQgghnn76adGsWTPx22+/iX379onY2FgRGxsrc9W3zmg0imbNmokXX3zRYnlDPX+FhYXi4MGD4uDBgwKAWLhwoTh48KB5ttCbb74p3NzcxI8//igOHz4sBg8eLEJCQkRpaal5G/379xcdO3YUu3fvFn/++acIDw8Xo0aNkuuQLNR2fHq9Xjz00EOiadOmIikpyeL/ZdXskp07d4r33ntPJCUliTNnzoivvvpKeHt7izFjxsh8ZNfUdoyFhYVi5syZIjExUZw7d05s2bJF3HPPPSI8PFyUlZWZt3Enn0Mhbv5zKoQQ+fn5wtHRUSxdurTa6+/083izzwchbv471GAwiHbt2om+ffuKpKQksWHDBuHt7S1mzZolWZ0MNxJZvHixaNasmVCr1SI6Olrs2rVL7pJuG4AaH8uWLRNCCJGSkiJ69OghPDw8hEajEc2bNxcvvPCCyM/Pl7fwOho5cqTw9/cXarVaBAQEiJEjR4rTp0+bny8tLRWTJ08W7u7uwtHRUQwdOlSkp6fLWPHt2bhxowAgTp06ZbG8oZ6/rVu31vhzOXbsWCFE5XTw1157Tfj6+gqNRiP69OlT7dgvX74sRo0aJZydnYWrq6sYP368KCwslOFoqqvt+M6dO3fD/5dbt24VQgixf/9+ERMTI3Q6ndBqtaJ169biX//6l0UwkFttx1hSUiL69u0rvL29hb29vQgKChJPPvlktT8S7+RzKMTNf06FEOLjjz8WDg4OIi8vr9rr7/TzeLPPByHq9jv0/PnzYsCAAcLBwUF4eXmJ559/XlRUVEhWp+JqsURERESNAsfcEBERUaPCcENERESNCsMNERERNSoMN0RERNSoMNwQERFRo8JwQ0RERI0Kww0RERE1Kgw3RHTXUygU+OGHH+Qug4gkwnBDRLIaN24cFApFtUf//v3lLo2IGig7uQsgIurfvz+WLVtmsUyj0chUDRE1dGy5ISLZaTQa+Pn5WTzc3d0BVHYZLV26FAMGDICDgwNCQ0Oxdu1ai9cfOXIE9913HxwcHODp6YlJkyahqKjIYp3PP/8cbdu2hUajgb+/P6ZOnWrxfE5ODoYOHQpHR0eEh4fjp59+su5BE5HVMNwQ0R3vtddew/Dhw3Ho0CGMHj0ajz76KE6cOAEAKC4uRr9+/eDu7o69e/dizZo12LJli0V4Wbp0KaZMmYJJkybhyJEj+Omnn9C8eXOLfcybNw+PPPIIDh8+jIEDB2L06NHIzc216XESkUQkuwUnEdFtGDt2rFCpVMLJycni8cYbbwghKu9C/PTTT1u8JiYmRjzzzDNCCCE++eQT4e7uLoqKiszP//LLL0KpVJrvKN2kSRPxyiuv3LAGAOLVV181f19UVCQAiPXr10t2nERkOxxzQ0Sy6927N5YuXWqxzMPDw/x1bGysxXOxsbFISkoCAJw4cQKRkZFwcnIyP9+tWzeYTCacOnUKCoUCaWlp6NOnT601tG/f3vy1k5MTXF1dkZWVdbuHREQyYrghItk5OTlV6yaSioODQ53Ws7e3t/heoVDAZDJZoyQisjKOuSGiO96uXbuqfd+6dWsAQOvWrXHo0CEUFxebn9+xYweUSiVatmwJFxcXBAcHIyEhwaY1E5F82HJDRLIrLy9HRkaGxTI7Ozt4eXkBANasWYNOnTqhe/fu+Prrr7Fnzx589tlnAIDRo0djzpw5GDt2LObOnYvs7GxMmzYNjz/+OHx9fQEAc+fOxdNPPw0fHx8MGDAAhYWF2LFjB6ZNm2bbAyUim2C4ISLZbdiwAf7+/hbLWrZsiZMnTwKonMm0atUqTJ48Gf7+/vjmm2/Qpk0bAICjoyM2btyI5557Dp07d4ajoyOGDx+OhQsXmrc1duxYlJWV4b333sPMmTPh5eWFESNG2O4AicimFEIIIXcRREQ3olAo8P3332PIkCFyl0JEDQTH3BAREVGjwnBDREREjQrH3BDRHY0950R0q9hyQ0RERI0Kww0RERE1Kgw3RERE1Kgw3BAREVGjwnBDREREjQrDDRERETUqDDdERETUqDDcEBERUaPCcENERESNyv8DL0ABiv0LSQoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSE scores: [0.023709578, 0.024433369, 0.023779787, 0.023855904, 0.023779416]\n",
            "Validation MSE scores: [0.024711017, 0.02185082, 0.02444857, 0.024111189, 0.024460683]\n",
            "Validation MAE scores: [0.12842454, 0.119229, 0.12845467, 0.12632318, 0.12915443]\n",
            "Mean Train MSE: 0.02391161024570465\n",
            "Mean Validation MSE: 0.02391645684838295\n",
            "Mean Validation MAE: 0.12631717324256897\n",
            "Best model path: best_model_fold_2.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation du modèle sur les données de test\n",
        "X_test_array = [X_test[:, 0], X_test[:, 1], content_test]\n",
        "test_predictions = best_model.predict(X_test_array)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "print(f\"Test MSE: {test_mse}\")\n",
        "print(f\"Test MAE: {test_mae}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57oaz56zTv0x",
        "outputId": "448cfb51-444e-471a-dd75-2c7de6261c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Test MSE: 0.023195801302790642\n",
            "Test MAE: 0.12413423508405685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenir les dimensions des caractéristiques des produits utilisées pendant l'entraînement\n",
        "trained_product_features_shape = best_model.input[2].shape[1]\n",
        "\n",
        "# Afficher les dimensions pour déboguer\n",
        "print(f\"Dimensions des caractéristiques des produits utilisées pendant l'entraînement : {trained_product_features_shape}\")\n",
        "print(f\"Dimensions des caractéristiques des produits actuelles : {product_features.shape[1]}\")\n",
        "\n",
        "# Si les dimensions ne correspondent pas, ajuster les caractéristiques des produits\n",
        "if product_features.shape[1] != trained_product_features_shape:\n",
        "    print(\"Les dimensions des caractéristiques des produits ne correspondent pas. Ajustement nécessaire.\")\n",
        "    # Sélectionner uniquement les colonnes utilisées pendant l'entraînement\n",
        "    product_features_adjusted = product_features.iloc[:, :trained_product_features_shape]\n",
        "else:\n",
        "    product_features_adjusted = product_features\n",
        "\n",
        "def recommender_system(user_id, model, n_recommendations=10):\n",
        "    # Encoder l'ID de l'utilisateur\n",
        "    encoded_user_id = user_enc.transform([user_id])\n",
        "    print(f\"ID utilisateur encodé: {encoded_user_id}\")\n",
        "\n",
        "    # Obtenir les produits vus par l'utilisateur\n",
        "    seen_products = list(df_rat[df_rat['user_id'] == user_id]['id_product'])\n",
        "    print(f\"L'utilisateur {user_id} a vu les produits: {seen_products}\")\n",
        "\n",
        "    # Obtenir les produits non vus\n",
        "    unseen_products = [i for i in range(n_produits) if i not in seen_products]\n",
        "    print(f\"Produits non vus pour l'utilisateur {user_id}: {unseen_products[:10]} (affichage des 10 premiers)\")\n",
        "\n",
        "    if not unseen_products:\n",
        "        print(f\"Aucun produit non vu pour l'utilisateur {user_id}.\")\n",
        "        return\n",
        "\n",
        "    # Créer les tableaux d'entrée pour le modèle\n",
        "    user_array = np.array([encoded_user_id[0]] * len(unseen_products))\n",
        "    product_array = np.array(unseen_products)\n",
        "    content_array = product_features_adjusted.loc[unseen_products].values.astype('float32')\n",
        "\n",
        "    # Faire des prédictions\n",
        "    model_input = [user_array, product_array, content_array]\n",
        "    predicted_ratings = model.predict(model_input)\n",
        "    print(f\"Notes prédites pour l'utilisateur {user_id}: {predicted_ratings[:10]} (affichage des 10 premiers)\")\n",
        "\n",
        "    # Obtenir les meilleures N recommandations\n",
        "    sorted_indices = np.argsort(predicted_ratings[:, 0])[::-1]\n",
        "    recommended_product_ids = [unseen_products[i] for i in sorted_indices[:n_recommendations]]\n",
        "    recommended_products = item_enc.inverse_transform(recommended_product_ids)\n",
        "\n",
        "    top_predicted_ratings = predicted_ratings[sorted_indices[:n_recommendations], 0]\n",
        "\n",
        "    # Convertir les notes prédites en pourcentage avec une mise à l'échelle appropriée\n",
        "    top_predicted_ratings_percentage = (top_predicted_ratings - np.min(top_predicted_ratings)) / (np.max(top_predicted_ratings) - np.min(top_predicted_ratings)) * 100\n",
        "\n",
        "    print(f\"Top {n_recommendations} recommandations de produits pour l'utilisateur {user_id}:\")\n",
        "    print(recommended_products)\n",
        "\n",
        "    # Obtenir les titres des produits recommandés\n",
        "    recommended_titles = df_pro[df_pro['id_product'].isin(recommended_products)]['Title'].values\n",
        "\n",
        "    # Générer une liste de couleurs\n",
        "    colors = list(mcolors.TABLEAU_COLORS.values())\n",
        "\n",
        "    # Afficher les résultats dans un graphique en barres avec des pourcentages et des couleurs différentes\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.barh(range(n_recommendations), top_predicted_ratings_percentage, color=colors[:n_recommendations], align='center')\n",
        "    plt.yticks(range(n_recommendations), recommended_titles)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel('Note Prédite (%)')\n",
        "    plt.title(f'Top {n_recommendations} Recommandations de Produits pour l\\'Utilisateur {user_id}')\n",
        "\n",
        "    # Ajouter des étiquettes de pourcentage aux barres\n",
        "    for bar, rating in zip(bars, top_predicted_ratings_percentage):\n",
        "        plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, f'{rating:.1f}%', va='center')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Exemple d'utilisation\n",
        "user_id_example = 450  # ID de l'utilisateur pour lequel vous souhaitez obtenir des recommandations\n",
        "n_recommendations = 10 # Nombre de produits à recommander\n",
        "recommender_system(user_id_example, best_model, n_recommendations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "m_RhtwKcp6gI",
        "outputId": "3ea4e57f-f96b-45ac-f7ee-5982e071c105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions des caractéristiques des produits utilisées pendant l'entraînement : 2793\n",
            "Dimensions des caractéristiques des produits actuelles : 2793\n",
            "ID utilisateur encodé: [450]\n",
            "L'utilisateur 450 a vu les produits: []\n",
            "Produits non vus pour l'utilisateur 450: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] (affichage des 10 premiers)\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Notes prédites pour l'utilisateur 450: [[2.0249496e-35]\n",
            " [4.2346774e-35]\n",
            " [1.3381313e-09]\n",
            " [5.3403248e-35]\n",
            " [3.9463702e-35]\n",
            " [3.8780914e-29]\n",
            " [4.1849463e-34]\n",
            " [4.8324609e-14]\n",
            " [2.5111108e-11]\n",
            " [8.9560157e-38]] (affichage des 10 premiers)\n",
            "Top 10 recommandations de produits pour l'utilisateur 450:\n",
            "[3784 3814 3783 3395 3649 3781 3238 3779 1181 1768]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}